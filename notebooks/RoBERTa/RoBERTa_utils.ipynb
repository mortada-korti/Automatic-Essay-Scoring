{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537abf84-427a-453e-a4bb-e1dc4e606ec7",
   "metadata": {},
   "source": [
    "# RoBERTa_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7c9b4-12be-44c5-90a3-465e2f7cadd9",
   "metadata": {},
   "source": [
    "### Import Core RoBERTa Components and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05ac14-bf78-4cef-b902-c2c6c0cc4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaSelfAttention,     # RoBERTa's multi-head self-attention layer\n",
    "    RobertaSelfOutput,        # Output processing after attention (residual + norm)\n",
    "    RobertaEmbeddings,        # Token + positional + segment embeddings\n",
    ")\n",
    "from transformers import RobertaModel, RobertaPreTrainedModel  \n",
    "import numpy as np        \n",
    "import torch                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4188c5e-284c-4f15-8f03-e97d59905a3c",
   "metadata": {},
   "source": [
    "### MoEFeedForward: Mixture-of-Experts FFN for RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7152e-a221-4569-904a-194c89be14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEFeedForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Mixture-of-Experts (MoE) feed-forward layer with optional sparse (top-k) expert selection.\n",
    "\n",
    "    Parameters:\n",
    "        hidden_dim (int): Input and output dimensionality.\n",
    "        intermediate_dim (int): Hidden size of each expert's feed-forward network.\n",
    "        num_experts (int): Number of expert networks to choose from.\n",
    "        dropout (float): Dropout probability applied to final output.\n",
    "        top_k (int): Number of top experts to route each token through (top-k gating).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, intermediate_dim, num_experts=7, dropout=0.2, top_k=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k  \n",
    "\n",
    "        # Define expert feed-forward networks: Linear → GELU → Linear\n",
    "        self.experts = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_dim, intermediate_dim),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(intermediate_dim, hidden_dim)\n",
    "            ) for expert in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        self.gate = torch.nn.Linear(hidden_dim, num_experts)  # Gating network to assign expert weights\n",
    "        self.dropout = torch.nn.Dropout(dropout)              # Dropout on the final combined output\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_logits = self.gate(x)  # Raw scores for expert selection: (batch, seq_len, num_experts)\n",
    "\n",
    "        if self.top_k > 0 and self.top_k < self.num_experts:\n",
    "            # Top-k sparse expert selection\n",
    "            topk_values, topk_indices = torch.topk(gate_logits, self.top_k, dim=-1)\n",
    "\n",
    "            # Create a mask with only top-k values retained\n",
    "            mask = torch.full_like(gate_logits, float('-inf'))\n",
    "            mask.scatter_(-1, topk_indices, topk_values)\n",
    "\n",
    "            # Normalize over top-k experts only\n",
    "            gate_weights = torch.nn.functional.softmax(mask, dim=-1)\n",
    "        else:\n",
    "            # Dense routing: softmax over all experts\n",
    "            gate_weights = torch.nn.functional.softmax(gate_logits, dim=-1)\n",
    "\n",
    "        # Apply each expert to input x\n",
    "        expert_outputs = [expert(x) for expert in self.experts]             # List of (batch, seq_len, hidden)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=2)                 # Shape: (batch, seq_len, num_experts, hidden)\n",
    "\n",
    "        gate_weights = gate_weights.unsqueeze(-1)                           # Shape: (batch, seq_len, num_experts, 1)\n",
    "        weighted_output = expert_outputs * gate_weights                    # Apply weights\n",
    "        output = weighted_output.sum(dim=2)                                 # Combine experts into one output\n",
    "\n",
    "        return self.dropout(output), gate_weights.squeeze(-1)              # Final output and gate weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dbb7ec-4c11-4742-a269-2382abb30179",
   "metadata": {},
   "source": [
    "### RobertaLayerWithMoE: MoE-Enhanced Transformer Layer for RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb663db-aba0-4434-bc3b-2bf2125ec0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaLayerWithMoE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A modified RoBERTa encoder layer where the standard feed-forward network (FFN)\n",
    "    is replaced by a Mixture-of-Experts (MoE) block.\n",
    "\n",
    "    Parameters:\n",
    "        config (RobertaConfig): Hugging Face configuration for RoBERTa.\n",
    "        num_experts (int): Number of parallel experts in the MoE layer.\n",
    "        top_k (int): Number of top experts to select per token (sparse routing).\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_experts=7, top_k=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = RobertaSelfAttention(config)           # Multi-head self-attention\n",
    "        self.attention_output = RobertaSelfOutput(config)       # Output projection + residual + norm\n",
    "\n",
    "        # Replace the default FFN with Mixture-of-Experts block\n",
    "        self.intermediate = MoEFeedForward(\n",
    "            num_experts=num_experts,\n",
    "            hidden_dim=config.hidden_size,\n",
    "            dropout=config.hidden_dropout_prob,\n",
    "            intermediate_dim=config.intermediate_size,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # Final dense projection and normalization\n",
    "        self.output_dense = torch.nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.output_dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.output_norm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def initialize_experts_from_ffn(self, pretrained_ffn):\n",
    "        \"\"\"\n",
    "        Initialize all experts in the MoE block using weights from a standard pretrained FFN.\n",
    "\n",
    "        Parameters:\n",
    "            pretrained_ffn (torch.nn.Sequential): A Linear → GELU → Linear module from RoBERTa.\n",
    "        \"\"\"\n",
    "        for expert in self.intermediate.experts:\n",
    "            # Copy weights from the standard FFN layers\n",
    "            expert[0].weight.data.copy_(pretrained_ffn[0].weight.data.clone())\n",
    "            expert[0].bias.data.copy_(pretrained_ffn[0].bias.data.clone())\n",
    "            expert[2].weight.data.copy_(pretrained_ffn[2].weight.data.clone())\n",
    "            expert[2].bias.data.copy_(pretrained_ffn[2].bias.data.clone())\n",
    "\n",
    "            # Add noise to slightly diversify expert parameters\n",
    "            for param in expert.parameters():\n",
    "                param.data += 0.01 * torch.randn_like(param)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the modified RoBERTa encoder layer.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_states (Tensor): Input tensor (batch, seq_len, hidden_dim).\n",
    "            attention_mask (Tensor, optional): Attention mask for padding tokens.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]:\n",
    "                - layer_output: Final hidden states after attention + MoE FFN + normalization\n",
    "                - gate_weights: Gating weights from MoE for each token\n",
    "        \"\"\"\n",
    "        attention_output = self.attention(hidden_states, attention_mask)[0]       # Self-attention\n",
    "        attention_output = self.attention_output(attention_output, hidden_states) # Residual + norm\n",
    "\n",
    "        moe_output, gate_weights = self.intermediate(attention_output)            # MoE FFN\n",
    "\n",
    "        ffn_output = self.output_dense(moe_output)\n",
    "        ffn_output = self.output_dropout(ffn_output)\n",
    "        layer_output = self.output_norm(ffn_output + attention_output)            # Final residual + norm\n",
    "\n",
    "        return layer_output, gate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffd0f8-dd4b-4ae0-a50f-d5be45951394",
   "metadata": {},
   "source": [
    "### MoeRobertaModel: Full RoBERTa Encoder with MoE Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf48cd5-7704-487f-9956-d7ccfe5094d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeRobertaModel(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Full RoBERTa model with Mixture-of-Experts (MoE) integrated into every encoder layer.\n",
    "\n",
    "    Replaces the standard feed-forward block in each layer with a MoE module and \n",
    "    optionally initializes each expert using pretrained FFN weights.\n",
    "\n",
    "    Parameters:\n",
    "        config (RobertaConfig): Configuration object.\n",
    "        num_experts (int): Number of experts per MoE layer.\n",
    "        top_k (int): Number of experts selected per token (top-k routing).\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_experts=7, top_k=4):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = RobertaEmbeddings(config)  # Token + position embeddings\n",
    "\n",
    "        # Replace standard encoder layers with MoE-enhanced versions\n",
    "        self.encoder_layers = torch.nn.ModuleList([\n",
    "            RobertaLayerWithMoE(config, num_experts=num_experts, top_k=top_k)\n",
    "            for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        self.init_weights()  # Initialize all model parameters\n",
    "\n",
    "        # Load base RoBERTa model to copy pretrained FFN weights into MoE experts\n",
    "        base_roberta = RobertaModel(config)\n",
    "        base_roberta.eval()\n",
    "\n",
    "        for i, moe_layer in enumerate(self.encoder_layers):\n",
    "            intermediate = base_roberta.encoder.layer[i].intermediate.dense\n",
    "            output = base_roberta.encoder.layer[i].output.dense\n",
    "\n",
    "            # Wrap original FFN for compatibility with expert structure\n",
    "            pretrained_ffn = torch.nn.Sequential(\n",
    "                intermediate,\n",
    "                torch.nn.GELU(),\n",
    "                output\n",
    "            )\n",
    "\n",
    "            # Initialize all MoE experts from this base FFN\n",
    "            moe_layer.initialize_experts_from_ffn(pretrained_ffn)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the entire MoE-enhanced RoBERTa encoder.\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (Tensor): Token IDs (batch_size, seq_len).\n",
    "            attention_mask (Tensor, optional): Padding mask.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, List[Tensor]]:\n",
    "                - hidden_states: Final hidden layer outputs\n",
    "                - pooled_output: First token (like [CLS]) used as global representation\n",
    "                - gate_weights_list: Gating weights from all MoE layers\n",
    "        \"\"\"\n",
    "        embedding_output = self.embeddings(input_ids=input_ids)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            extended_attention_mask = attention_mask[:, None, None, :]  # (batch, 1, 1, seq_len)\n",
    "            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        else:\n",
    "            extended_attention_mask = None\n",
    "\n",
    "        hidden_states = embedding_output\n",
    "        gate_weights_list = []  # Track gate weights layer by layer\n",
    "\n",
    "        # Pass through all encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states, gate_weights = layer(hidden_states, attention_mask=extended_attention_mask)\n",
    "            gate_weights_list.append(gate_weights)\n",
    "\n",
    "        pooled_output = hidden_states[:, 0]  # Use first token ([CLS]) as summary\n",
    "\n",
    "        return hidden_states, pooled_output, gate_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80024194-ad5f-4379-b7f1-db494eeb4b05",
   "metadata": {},
   "source": [
    "###  MoeRobertaScorer: Essay Scoring Head with MoE Entropy Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c72a2-be3d-4b92-acd3-0d0c4c0dc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeRobertaScorer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Regression head for MoE-enhanced RoBERTa. Supports:\n",
    "    - Handcrafted feature fusion\n",
    "    - Entropy-based auxiliary loss for expert regularization\n",
    "    - Expert usage diagnostics\n",
    "\n",
    "    Parameters:\n",
    "        base_model (MoeRobertaModel): MoE-based encoder.\n",
    "        dropout (float): Dropout rate in the regression head.\n",
    "        feature_dim (int): Size of optional handcrafted feature vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: MoeRobertaModel, dropout=0.2, feature_dim=0):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model                     # MoE-RoBERTa encoder\n",
    "        self.feature_dim = feature_dim                # Optional handcrafted feature dimension\n",
    "\n",
    "        input_dim = self.encoder.config.hidden_size + feature_dim  # Final input size to regressor\n",
    "\n",
    "        # Regression head: outputs scalar prediction\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 1),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, features=None, labels=None, aux_loss_weight=0.5):\n",
    "        \"\"\"\n",
    "        Forward pass through the scoring model.\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (Tensor): Token IDs.\n",
    "            attention_mask (Tensor): Binary attention mask (1 = real token).\n",
    "            features (Tensor, optional): External handcrafted features.\n",
    "            labels (Tensor, optional): Ground truth scores for regression.\n",
    "            aux_loss_weight (float): Weight of entropy-based auxiliary loss.\n",
    "\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'loss': total loss (MSE + entropy),\n",
    "                'logits': predicted scores,\n",
    "                'hidden_states': final encoder outputs,\n",
    "                'aux_loss': entropy-based auxiliary term\n",
    "            }\n",
    "        \"\"\"\n",
    "        hidden_states, pooled_output, gate_weights_list = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        last_gate = gate_weights_list[-1]  # Final layer gate weights\n",
    "\n",
    "        # Calculate expert usage stats if regularization is enabled\n",
    "        if aux_loss_weight > 0 and last_gate.ndim == 3:\n",
    "            usage_mask = (last_gate > 0).float()                   # Binary mask of active experts\n",
    "            usage_count = usage_mask.sum(dim=(0, 1))               # Expert usage count\n",
    "            self.expert_usage_counts = usage_count.detach().cpu()\n",
    "\n",
    "            prob_dist = self.expert_usage_counts / self.expert_usage_counts.sum()  # Normalize\n",
    "            self.expert_entropy = -(prob_dist * torch.log(prob_dist + 1e-8)).sum().item()  # Entropy\n",
    "\n",
    "        # Track average gate distribution (for logging/visualization)\n",
    "        self.last_gate_weights = gate_weights_list[-1].mean(dim=1).detach().cpu()\n",
    "\n",
    "        # If handcrafted features are used, concatenate with pooled output\n",
    "        if features is not None:\n",
    "            pooled_output = torch.cat([pooled_output, features], dim=-1)\n",
    "\n",
    "        # Predict final score\n",
    "        score = self.regressor(pooled_output).squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.functional.mse_loss(score, labels.float())  # Core regression loss\n",
    "\n",
    "            if gate_weights_list:\n",
    "                gate_weights = gate_weights_list[-1]\n",
    "                mean_gates = gate_weights.mean(dim=(0, 1))              # Average over batch + seq\n",
    "                entropy = -(mean_gates * torch.log(mean_gates + 1e-9)).sum()\n",
    "                aux_loss = -entropy\n",
    "\n",
    "                # Combine MSE loss with MoE auxiliary entropy loss\n",
    "                loss = loss + aux_loss_weight * aux_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": score,\n",
    "            \"hidden_states\": hidden_states,\n",
    "            \"aux_loss\": aux_loss if labels is not None else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c6a1a-a692-4179-8714-3b705064040b",
   "metadata": {},
   "source": [
    "###  freeze_roberta_layers: Fine-Tuning Only the Top RoBERTa Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b6189-14c1-4642-8a8e-1abe5a03888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_roberta_layers(model, num_unfrozen=2):\n",
    "    \"\"\"\n",
    "    Freezes all layers of a RoBERTa-based model except the last `num_unfrozen` encoder layers.\n",
    "\n",
    "    This is useful in transfer learning scenarios where:\n",
    "    - You want to retain pretrained knowledge from lower layers\n",
    "    - You want to reduce computational cost or memory usage\n",
    "    - You only need high-level adaptation to your downstream task\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The RoBERTa or MoeRoberta model.\n",
    "        num_unfrozen (int): Number of top encoder layers to unfreeze for training.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # Freeze all layers by default\n",
    "\n",
    "    # Unfreeze only the last `num_unfrozen` encoder layers\n",
    "    if hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):\n",
    "        total_layers = len(model.encoder.layer)\n",
    "        for i in range(total_layers - num_unfrozen, total_layers):\n",
    "            for param in model.encoder.layer[i].parameters():\n",
    "                param.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aes_env]",
   "language": "python",
   "name": "conda-env-aes_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
