{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96974991-a024-4ec1-9e5b-6f86a3e9383a",
   "metadata": {},
   "source": [
    "# BERT_utils_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cb1a4-eff8-4765-ac42-0d741e5ba2e6",
   "metadata": {},
   "source": [
    "### Import Core BERT Components and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These bring in ready-made BERT components and utilities from Hugging Face's Transformers library.\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertSelfAttention,  # The multi-head self-attention mechanism used in BERT layers\n",
    "    BertSelfOutput,     # Post-attention dense + dropout + residual connection\n",
    "    BertEmbeddings,     # Word, position, and segment embeddings for BERT inputs\n",
    "    BertPooler          # The pooling layer that produces a fixed-size vector from BERT's output\n",
    ")\n",
    "\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "# BertModel        → Full pretrained BERT architecture (encoder only)\n",
    "# BertPreTrainedModel → Base class that handles loading, saving, and config management\n",
    "\n",
    "import torch  # PyTorch library for building and training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5520a46-bc75-4bde-a788-1bfefbd7ca33",
   "metadata": {},
   "source": [
    "### Mixture-of-Experts Feed-Forward Layer for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MoEFeedForward ---\n",
    "# Purpose:\n",
    "# Implements a \"Mixture of Experts\" feed-forward network.\n",
    "# Each token's representation is processed by multiple expert networks.\n",
    "# A small gating network decides how much each expert contributes (top-k selection possible).\n",
    "\n",
    "class MoEFeedForward(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, intermediate_dim, num_experts=7, dropout=0.2, top_k=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim                # Input/output feature size\n",
    "        self.intermediate_dim = intermediate_dim    # Hidden size inside each expert\n",
    "        self.num_experts = num_experts              # Total experts available\n",
    "        self.top_k = top_k                          # Number of experts to use per token\n",
    "\n",
    "        # Gate network: predicts weights for each expert based on input\n",
    "        self.gate = torch.nn.Linear(hidden_dim, num_experts)\n",
    "\n",
    "        # Dropout to reduce overfitting\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Create the list of expert feed-forward networks\n",
    "        self.experts = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_dim, intermediate_dim),  # First dense layer\n",
    "                torch.nn.GELU(),                                # Activation function\n",
    "                torch.nn.Linear(intermediate_dim, hidden_dim)   # Second dense layer\n",
    "            ) for expert in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, expert_mask=None):     \n",
    "        # Get gate scores for each expert (before softmax)\n",
    "        gate_logits = self.gate(x)\n",
    "\n",
    "        if 0 < self.top_k < self.num_experts:\n",
    "            # Select the top-k experts per token\n",
    "            topk_values, topk_indices = torch.topk(gate_logits, self.top_k, dim=-1)\n",
    "\n",
    "            # Initialize all as -inf (so non-top-k experts get zero weight after softmax)\n",
    "            masked = torch.full_like(gate_logits, float(\"-inf\"))\n",
    "\n",
    "            # Fill only top-k positions with their scores\n",
    "            masked.scatter_(-1, topk_indices, topk_values)\n",
    "\n",
    "            # Stabilize values by subtracting the max in each row\n",
    "            masked = masked - masked.amax(dim=-1, keepdim=True)\n",
    "\n",
    "            # Softmax to turn scores into probabilities\n",
    "            gate_weights = torch.nn.functional.softmax(masked, dim=-1)\n",
    "        else:\n",
    "            # Use all experts — just stabilize then softmax\n",
    "            logits = gate_logits - gate_logits.amax(dim=-1, keepdim=True)\n",
    "            gate_weights = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Run all experts and stack their outputs: shape (batch, seq_len, num_experts, hidden_dim)\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=2)\n",
    "\n",
    "        # Weighted sum of experts based on gate probabilities\n",
    "        output = (expert_outputs * gate_weights.unsqueeze(-1)).sum(dim=2)\n",
    "\n",
    "        # Apply dropout and return both the output and the gate weights\n",
    "        return self.dropout(output), gate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df28fa2-4b48-4044-b0f4-9d0d0804b1ee",
   "metadata": {},
   "source": [
    "### Custom BERT Layer with Mixture-of-Experts (MoE) Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BertLayerWithMoE ---\n",
    "# Purpose:\n",
    "# A modified BERT encoder layer where the usual feed-forward network (FFN)\n",
    "# is replaced with the Mixture of Experts (MoE) feed-forward layer.\n",
    "# Still keeps the standard BERT self-attention, residual connections, and layer normalization.\n",
    "\n",
    "class BertLayerWithMoE(torch.nn.Module):\n",
    "    def __init__(self, config, num_experts=7, top_k=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Standard multi-head self-attention module\n",
    "        self.attention = BertSelfAttention(config)\n",
    "\n",
    "        # Output stage after attention: dense + dropout + residual\n",
    "        self.attention_output = BertSelfOutput(config)\n",
    "\n",
    "        # Replace the standard FFN with the MoE FFN\n",
    "        self.intermediate = MoEFeedForward(\n",
    "            num_experts=num_experts,                     # total experts\n",
    "            hidden_dim=config.hidden_size,               # input/output size\n",
    "            dropout=config.hidden_dropout_prob,          # dropout rate\n",
    "            intermediate_dim=config.intermediate_size,   # hidden size in each expert\n",
    "            top_k=top_k                                  # how many experts to pick per token\n",
    "        )\n",
    "\n",
    "        # Dropout after MoE output\n",
    "        self.output_dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # Layer normalization after combining MoE output with attention output\n",
    "        self.output_norm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def initialize_experts_from_ffn(self, pretrained_ffn):\n",
    "        # Copy weights from an existing (pretrained) FFN into all experts\n",
    "        for expert in self.intermediate.experts:\n",
    "            # First linear layer weights & bias\n",
    "            expert[0].weight.data.copy_(pretrained_ffn[0].weight.data.clone())\n",
    "            expert[0].bias.data.copy_(pretrained_ffn[0].bias.data.clone())\n",
    "\n",
    "            # Second linear layer weights & bias\n",
    "            expert[2].weight.data.copy_(pretrained_ffn[2].weight.data.clone())\n",
    "            expert[2].bias.data.copy_(pretrained_ffn[2].bias.data.clone())\n",
    "\n",
    "            # Add small random noise so experts can diversify during training\n",
    "            noise_scale = 0.0 if len(self.intermediate.experts) == 1 else 0.0\n",
    "            for param in expert.parameters():\n",
    "                param.data.add_(noise_scale * torch.randn_like(param))\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, expert_mask=None):\n",
    "        # Apply self-attention to input\n",
    "        attention_output = self.attention(hidden_states, attention_mask)[0]\n",
    "\n",
    "        # Apply attention output projection + dropout + residual\n",
    "        attention_output = self.attention_output(attention_output, hidden_states)\n",
    "\n",
    "        # Pass through the MoE feed-forward network\n",
    "        moe_output, gate_weights = self.intermediate(attention_output, expert_mask=expert_mask)\n",
    "\n",
    "        # Apply dropout to MoE output\n",
    "        ffn_output = self.output_dropout(moe_output)\n",
    "\n",
    "        # Add residual connection from attention output and normalize\n",
    "        layer_output = self.output_norm(ffn_output + attention_output)\n",
    "\n",
    "        # Return the final layer output and the expert gate weights\n",
    "        return layer_output, gate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f9e34-0d43-4d85-8f65-5f43387fbad3",
   "metadata": {},
   "source": [
    "### MoeBERTModel: Full BERT Encoder with MoE-Enhanced Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35875ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MoeBERTModel ---\n",
    "# Purpose:\n",
    "# A custom BERT model where every transformer layer's feed-forward network (FFN)\n",
    "# is replaced by a Mixture-of-Experts (MoE) version.\n",
    "# Starts from a pretrained BERT, reuses its embeddings, pooler, and attention layers,\n",
    "# then initializes MoE experts from the pretrained FFN weights.\n",
    "\n",
    "class MoeBERTModel(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_experts=7, top_k=2, pretrained_name_or_path=\"bert-base-uncased\"):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_experts = num_experts  # total experts per layer\n",
    "        self.top_k = top_k              # number of active experts per token\n",
    "\n",
    "        # Load pretrained BERT model (for initialization)\n",
    "        base_bert = BertModel.from_pretrained(pretrained_name_or_path, config=config)\n",
    "        base_bert.eval()  # disable dropout during weight copying\n",
    "\n",
    "        # Reuse pretrained embedding and pooler layers\n",
    "        self.embeddings = base_bert.embeddings\n",
    "        self.pooler = base_bert.pooler\n",
    "\n",
    "        # Create MoE-based transformer layers\n",
    "        self.encoder_layers = torch.nn.ModuleList([\n",
    "            BertLayerWithMoE(config, num_experts=num_experts, top_k=top_k)\n",
    "            for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        # Initialize MoE layers from the pretrained FFN weights\n",
    "        with torch.no_grad():\n",
    "            for i, moe_layer in enumerate(self.encoder_layers):\n",
    "                base_layer = base_bert.encoder.layer[i]\n",
    "\n",
    "                # Reuse attention sublayers directly from pretrained BERT\n",
    "                moe_layer.attention = base_layer.attention.self\n",
    "                moe_layer.attention_output = base_layer.attention.output\n",
    "\n",
    "                # Copy LayerNorm weights from pretrained\n",
    "                moe_layer.output_norm.weight.copy_(base_layer.output.LayerNorm.weight)\n",
    "                moe_layer.output_norm.bias.copy_(base_layer.output.LayerNorm.bias)\n",
    "\n",
    "                # Create a reference FFN sequence from pretrained layer\n",
    "                pretrained_ffn = torch.nn.Sequential(\n",
    "                    base_layer.intermediate.dense,\n",
    "                    torch.nn.GELU(),\n",
    "                    base_layer.output.dense\n",
    "                )\n",
    "\n",
    "                # Initialize each MoE expert from the pretrained FFN\n",
    "                moe_layer.initialize_experts_from_ffn(pretrained_ffn)\n",
    "\n",
    "        # Store last gate weights for inspection\n",
    "        self._last_gate_weights = None\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, expert_mask=None):\n",
    "        device = input_ids.device\n",
    "        extended_attention_mask = None\n",
    "\n",
    "        # Prepare the attention mask for BERT (broadcasts to all heads)\n",
    "        if attention_mask is not None:\n",
    "            extended_attention_mask = self.get_extended_attention_mask(\n",
    "                attention_mask, input_ids.shape, device\n",
    "            )\n",
    "\n",
    "        # Move expert mask to correct device if provided\n",
    "        if expert_mask is not None:\n",
    "            expert_mask = expert_mask.to(device)\n",
    "\n",
    "        # Run the embedding layer\n",
    "        hidden_states = self.embeddings(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        gate_weights_list = []  # store gating info from each layer\n",
    "\n",
    "        # Pass through all MoE transformer layers\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states, gate_weights = layer(\n",
    "                hidden_states,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                expert_mask=expert_mask\n",
    "            )\n",
    "            gate_weights_list.append(gate_weights)\n",
    "\n",
    "        # Pooling: mean over valid tokens if mask provided, else use [CLS] token\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(-1).to(hidden_states.dtype)  # (B, T, 1)\n",
    "            summed = (hidden_states * mask).sum(dim=1)                   # sum over tokens\n",
    "            denom = mask.sum(dim=1).clamp_min(1e-6)                      # avoid div by zero\n",
    "            pooled_output = summed / denom\n",
    "        else:\n",
    "            pooled_output = hidden_states[:, 0]  # CLS token\n",
    "\n",
    "        # Save gating info for later inspection\n",
    "        self._last_gate_weights = gate_weights_list\n",
    "\n",
    "        return hidden_states, pooled_output, gate_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1bf91-344e-41e5-8e85-cc10bff7568b",
   "metadata": {},
   "source": [
    "### MoeBERTScorer: Essay Scoring Head with Optional Handcrafted Features & MoE Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99436e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- MoeBERTScorer ---\n",
    "# Purpose:\n",
    "# Wraps a MoeBERTModel to produce a single scalar score (e.g., for regression tasks).\n",
    "# Can take optional extra features, track expert usage stats, and apply extra loss terms\n",
    "# to supervise or regularize expert gating behavior.\n",
    "\n",
    "class MoeBERTScorer(torch.nn.Module):\n",
    "    def __init__(self, base_model: MoeBERTModel, dropout=0.2, feature_dim=0):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model                # The underlying BERT+MoE model\n",
    "        self.feature_dim = feature_dim            # Size of extra input features\n",
    "\n",
    "        # Total input size to regressor = BERT output size + any extra feature size\n",
    "        input_dim = self.encoder.config.hidden_size + feature_dim\n",
    "\n",
    "        # Simple regressor: linear projection to 1 value + dropout\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 1),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # For logging/debugging expert behavior\n",
    "        self.last_gate_weights = None\n",
    "        self.expert_usage_counts = None\n",
    "        self.expert_entropy = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        features=None,\n",
    "        labels=None,\n",
    "        aux_loss_weight=0.5,\n",
    "        expert_mask=None\n",
    "    ):\n",
    "        # Encode inputs using the MoE BERT model\n",
    "        hidden_states, pooled_output, gate_weights_list = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            expert_mask=expert_mask\n",
    "        )\n",
    "\n",
    "        # Last layer's gating probabilities\n",
    "        last_gate = gate_weights_list[-1] \n",
    "\n",
    "        # ---- Optional: Track expert usage + entropy ----\n",
    "        if aux_loss_weight is not None and aux_loss_weight > 0 and last_gate.ndim == 3:\n",
    "            with torch.no_grad():\n",
    "                prob_mass = last_gate.sum(dim=(0, 1))  # total usage per expert across batch/tokens\n",
    "                self.expert_usage_counts = prob_mass.detach().cpu()\n",
    "\n",
    "                pm_sum = prob_mass.sum()\n",
    "                if pm_sum > 0:\n",
    "                    # Normalize to distribution and compute entropy\n",
    "                    prob_dist = (prob_mass / (pm_sum + 1e-12)).clamp_min(1e-12)\n",
    "                    self.expert_entropy = float(-(prob_dist * prob_dist.log()).sum().item())\n",
    "                else:\n",
    "                    self.expert_entropy = 0.0\n",
    "\n",
    "        # Save average gate weights for inspection\n",
    "        with torch.no_grad():\n",
    "            self.last_gate_weights = last_gate.mean(dim=1).detach().cpu()\n",
    "\n",
    "        # ---- Append extra features if provided ----\n",
    "        if features is not None:\n",
    "            if features.device != pooled_output.device:\n",
    "                features = features.to(pooled_output.device)\n",
    "            if features.dtype != pooled_output.dtype:\n",
    "                features = features.to(pooled_output.dtype)\n",
    "            pooled_output = torch.cat([pooled_output, features], dim=-1)\n",
    "\n",
    "        # ---- Predict score ----\n",
    "        score = self.regressor(pooled_output).squeeze(-1)  # (B,)\n",
    "\n",
    "        # ---- Loss computation ----\n",
    "        loss = None\n",
    "        aux_loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            labels = labels.to(score.dtype)\n",
    "\n",
    "            # Base regression loss\n",
    "            loss = torch.nn.functional.mse_loss(score, labels)\n",
    "\n",
    "            # Auxiliary entropy regularization loss (discourage low-entropy gate usage)\n",
    "            if gate_weights_list and aux_loss_weight is not None and aux_loss_weight > 0:\n",
    "                gate_weights = gate_weights_list[-1]  \n",
    "                mean_gates = gate_weights.mean(dim=(0, 1))  \n",
    "                mean_gates = mean_gates / (mean_gates.sum() + 1e-12)\n",
    "                mean_gates = mean_gates.clamp_min(1e-12)\n",
    "                entropy = -(mean_gates * mean_gates.log()).sum()\n",
    "                aux_loss = -entropy  \n",
    "                loss = loss + aux_loss_weight * aux_loss\n",
    "\n",
    "        # ---- Return outputs ----\n",
    "        return {\n",
    "            \"loss\": loss,                    # total loss (if labels given)\n",
    "            \"logits\": score,                  # predicted scores\n",
    "            \"hidden_states\": hidden_states,   # encoder hidden states\n",
    "            \"aux_loss\": aux_loss if labels is not None else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772dfe5-7983-4b34-a78a-2ee9e1fc0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- preprocess ---\n",
    "# Purpose:\n",
    "# Converts a raw essay example into tokenized BERT inputs plus extra features and labels.\n",
    "# - Tokenizes the essay text.\n",
    "# - Adds the normalized score as the regression label.\n",
    "# - Collects handcrafted numerical features.\n",
    "# - Keeps track of which essay set it belongs to.\n",
    "\n",
    "def preprocess(example, tokenizer):\n",
    "    # Tokenize the essay text with fixed length (truncate/pad to 512 tokens)\n",
    "    tokens = tokenizer(\n",
    "        example[\"essay\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    # Add the normalized score as the label for training\n",
    "    tokens[\"labels\"] = float(example[\"normalized_score\"])\n",
    "\n",
    "    # Prefixes for handcrafted features we want to extract\n",
    "    feature_prefixes = (\"len_\", \"read_\", \"comp_\", \"var_\", \"sent_\")\n",
    "\n",
    "    # Collect all (key, value) pairs that start with those prefixes\n",
    "    feat_items = [(k, v) for k, v in example.items() if k.startswith(feature_prefixes)]\n",
    "\n",
    "    # Sort features by their key name to ensure consistent ordering\n",
    "    feat_items.sort(key=lambda kv: kv[0])  \n",
    "\n",
    "    # Convert feature values to floats\n",
    "    handcrafted_feats = [float(v) for _, v in feat_items]\n",
    "\n",
    "    # Store features in tokens dictionary\n",
    "    tokens[\"features\"] = handcrafted_feats\n",
    "\n",
    "    # Store essay set ID as an integer\n",
    "    tokens[\"essay_set\"] = int(example[\"essay_set\"])\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d19ff-8550-41b6-8fdd-b52ddc7a1e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- map_essay_set_to_expert ---\n",
    "# Purpose:\n",
    "# Creates a mapping from essay set IDs to expert indices.\n",
    "# Ensures mapping is consistent by sorting the set IDs first.\n",
    "\n",
    "def map_essay_set_to_expert(train_sets):\n",
    "    # Enumerate over sorted essay set IDs and assign each one a unique index\n",
    "    return {eid: idx for idx, eid in enumerate(sorted(train_sets))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00f652-2698-4966-b779-bc1bf364dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- add_expert_mask ---\n",
    "# Purpose:\n",
    "# Assigns an expert index to an example based on its essay set ID.\n",
    "# If the essay set is not in the map, assigns None.\n",
    "\n",
    "def add_expert_mask(example, expert_map):\n",
    "    # Get essay set ID (or None if missing)\n",
    "    es = example.get(\"essay_set\", None)\n",
    "\n",
    "    # If the essay set exists in the map, use its expert index\n",
    "    if es in expert_map:\n",
    "        example[\"expert_mask\"] = expert_map[es]\n",
    "    else:\n",
    "        # If not found, mark expert mask as None\n",
    "        example[\"expert_mask\"] = None\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a60f2c-791f-4436-8bf4-802e9d4bba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- data_collator ---\n",
    "# Purpose:\n",
    "# Custom batch collation function for Hugging Face Trainer.\n",
    "# Converts a list of feature dictionaries into a batch of PyTorch tensors,\n",
    "# handling both standard BERT inputs and additional custom fields.\n",
    "\n",
    "def data_collator(features):\n",
    "    batch = {}  # will store the batched tensors\n",
    "\n",
    "    # Batch token IDs if present\n",
    "    if \"input_ids\" in features[0]:\n",
    "        batch[\"input_ids\"] = torch.as_tensor([f[\"input_ids\"] for f in features], dtype=torch.long)\n",
    "\n",
    "    # Batch attention masks if present\n",
    "    if \"attention_mask\" in features[0]:\n",
    "        batch[\"attention_mask\"] = torch.as_tensor([f[\"attention_mask\"] for f in features], dtype=torch.long)\n",
    "\n",
    "    # Batch token type IDs if present and not None\n",
    "    if \"token_type_ids\" in features[0] and features[0][\"token_type_ids\"] is not None:\n",
    "        batch[\"token_type_ids\"] = torch.as_tensor([f[\"token_type_ids\"] for f in features], dtype=torch.long)\n",
    "\n",
    "    # Batch labels (float for regression)\n",
    "    if \"labels\" in features[0]:\n",
    "        batch[\"labels\"] = torch.as_tensor([f[\"labels\"] for f in features], dtype=torch.float32)\n",
    "\n",
    "    # Batch handcrafted features if they exist and are non-empty\n",
    "    if \"features\" in features[0] and features[0][\"features\"] is not None:\n",
    "        first_feats = features[0][\"features\"]\n",
    "        if isinstance(first_feats, (list, tuple)) and len(first_feats) > 0:\n",
    "            batch[\"features\"] = torch.as_tensor([f[\"features\"] for f in features], dtype=torch.float32)\n",
    "\n",
    "    # Batch expert mask if all examples have it\n",
    "    if \"expert_mask\" in features[0]:\n",
    "        mask_vals = [f[\"expert_mask\"] for f in features]\n",
    "        if all(m is not None for m in mask_vals):\n",
    "            batch[\"expert_mask\"] = torch.as_tensor(mask_vals, dtype=torch.long)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4752676-d53c-4bd5-8bb4-648eb71d49e4",
   "metadata": {},
   "source": [
    "### Freeze BERT Layers Except the Last Few (Fine-Tuning Strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ec301",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- freeze_bert_layers ---\n",
    "# Purpose:\n",
    "# Freezes all model parameters except for:\n",
    "# - The last `num_unfrozen` encoder layers\n",
    "# - The pooler (if present)\n",
    "# - The regressor head (if present)\n",
    "# Useful for fine-tuning only the top layers while keeping most of BERT fixed.\n",
    "\n",
    "def freeze_bert_layers(model, num_unfrozen=2):\n",
    "    # Freeze all parameters in the model\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Get the encoder module (may be directly the model or inside .encoder)\n",
    "    encoder = getattr(model, \"encoder\", model)\n",
    "\n",
    "    # Unfreeze the last `num_unfrozen` encoder layers\n",
    "    if hasattr(encoder, \"encoder_layers\"):\n",
    "        total_layers = len(encoder.encoder_layers)\n",
    "        start = max(0, total_layers - int(num_unfrozen))\n",
    "        for i in range(start, total_layers):\n",
    "            for p in encoder.encoder_layers[i].parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    # Unfreeze the pooler if it exists\n",
    "    if hasattr(encoder, \"pooler\") and encoder.pooler is not None:\n",
    "        for p in encoder.pooler.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # Unfreeze the regressor head if it exists\n",
    "    if hasattr(model, \"regressor\") and model.regressor is not None:\n",
    "        for p in model.regressor.parameters():\n",
    "            p.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python [conda env:aes_env]",
   "language": "python",
   "name": "conda-env-aes_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
