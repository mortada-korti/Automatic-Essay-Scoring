{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1647d13e-ff33-440d-9e00-5fb13709fd43",
   "metadata": {},
   "source": [
    "# BERT_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f66176-764f-4920-9927-c40fcf2b7696",
   "metadata": {},
   "source": [
    "### Import Core BERT Components and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05ac14-bf78-4cef-b902-c2c6c0cc4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertSelfAttention,      # The self-attention mechanism used inside BERT layers\n",
    "    BertSelfOutput,         # Applies a dense layer + residual connection + layer norm after attention\n",
    "    BertEmbeddings,         # Converts input token IDs to embeddings (token + segment + position)\n",
    "    BertPooler              # Produces a single pooled output from the final hidden state\n",
    ")\n",
    "from transformers import BertModel, BertPreTrainedModel  # Base BERT model and abstract class for custom BERT extensions\n",
    "import numpy as np     \n",
    "import torch           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50d7c7-10c6-4749-8cdd-dc99f2592a70",
   "metadata": {},
   "source": [
    "### Mixture-of-Experts Feed-Forward Layer for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7152e-a221-4569-904a-194c89be14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEFeedForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Mixture-of-Experts (MoE) feed-forward layer that replaces the \n",
    "    standard FFN in BERT. Multiple expert networks process the input, and a gating \n",
    "    mechanism determines how to combine their outputs.\n",
    "\n",
    "    Parameters:\n",
    "        hidden_dim (int): Input and output dimension (usually the BERT hidden size).\n",
    "        intermediate_dim (int): Hidden size within each expert's feed-forward network.\n",
    "        num_experts (int): Total number of parallel expert networks.\n",
    "        dropout (float): Dropout probability applied to the final output.\n",
    "        top_k (int): Number of top experts to select (sparse gating); if set to 0, use all experts.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, intermediate_dim, num_experts=7, dropout=0.2, top_k=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k  \n",
    "\n",
    "        # Define the expert networks â€” each one is a small feed-forward block\n",
    "        self.experts = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_dim, intermediate_dim),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(intermediate_dim, hidden_dim)\n",
    "            ) for expert in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # Gating network: assigns weights to each expert based on input\n",
    "        self.gate = torch.nn.Linear(hidden_dim, num_experts)\n",
    "\n",
    "        # Dropout applied to final output\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MoE layer.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: \n",
    "                - Output tensor of the same shape as input after combining expert outputs.\n",
    "                - Gating weights tensor (used for interpretability or diagnostics).\n",
    "        \"\"\"\n",
    "        gate_logits = self.gate(x)  # Compute unnormalized scores for each expert\n",
    "\n",
    "        # Use top-k gating to select a few experts per token\n",
    "        if self.top_k > 0 and self.top_k < self.num_experts:\n",
    "            topk_values, topk_indices = torch.topk(gate_logits, self.top_k, dim=-1)  # (batch, seq_len, top_k)\n",
    "\n",
    "            # Create a full mask and fill in top-k positions with their logits\n",
    "            mask = torch.full_like(gate_logits, float('-inf')) \n",
    "            mask.scatter_(-1, topk_indices, topk_values)\n",
    "\n",
    "            # Apply softmax to get normalized gating weights (only top-k will be nonzero)\n",
    "            gate_weights = torch.nn.functional.softmax(mask, dim=-1)\n",
    "        else:\n",
    "            # If top_k is 0 or equal to number of experts, use all experts\n",
    "            gate_weights = torch.nn.functional.softmax(gate_logits, dim=-1)\n",
    "\n",
    "        # Compute outputs from each expert\n",
    "        expert_outputs = [expert(x) for expert in self.experts]  # List of (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # Stack expert outputs: shape becomes (batch, seq_len, num_experts, hidden_dim)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=2)\n",
    "\n",
    "        # Reshape gate weights to align for broadcasting: (batch, seq_len, num_experts, 1)\n",
    "        gate_weights = gate_weights.unsqueeze(-1)\n",
    "\n",
    "        # Multiply each expert's output by its corresponding gate weight\n",
    "        weighted_output = expert_outputs * gate_weights  # (batch, seq_len, num_experts, hidden_dim)\n",
    "\n",
    "        # Sum over experts to get final output per token\n",
    "        output = weighted_output.sum(dim=2)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        return self.dropout(output), gate_weights.squeeze(-1)  # Also return gate weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff4cba-78f2-444f-a649-6eea73dbde14",
   "metadata": {},
   "source": [
    "### Custom BERT Layer with Mixture-of-Experts (MoE) Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb663db-aba0-4434-bc3b-2bf2125ec0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerWithMoE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A custom BERT layer that replaces the standard feed-forward sublayer with a \n",
    "    Mixture-of-Experts module. It preserves the original BERT attention mechanism\n",
    "    while injecting MoE routing for better flexibility and parameter efficiency.\n",
    "\n",
    "    Parameters:\n",
    "        config (BertConfig): Standard configuration object for BERT.\n",
    "        num_experts (int): Number of expert FFN modules in the MoE block.\n",
    "        top_k (int): Number of experts to activate per token (sparse routing).\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_experts=7, top_k=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # BERT's original multi-head self-attention mechanism\n",
    "        self.attention = BertSelfAttention(config)\n",
    "        self.attention_output = BertSelfOutput(config)\n",
    "\n",
    "        # Replace the feed-forward network with a MoE module\n",
    "        self.intermediate = MoEFeedForward(  \n",
    "            num_experts=num_experts,\n",
    "            hidden_dim=config.hidden_size,\n",
    "            dropout=config.hidden_dropout_prob,\n",
    "            intermediate_dim=config.intermediate_size,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # Final projection and normalization (as in original BERT layer)\n",
    "        self.output_dense = torch.nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.output_dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.output_norm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def initialize_experts_from_ffn(self, pretrained_ffn):\n",
    "        \"\"\"\n",
    "        Initialize all experts in the MoE block by copying weights from a standard \n",
    "        pretrained FFN (e.g., from a vanilla BERT model). Adds small noise to break symmetry.\n",
    "\n",
    "        Parameters:\n",
    "            pretrained_ffn (torch.nn.Sequential): A standard FFN block with 2 Linear layers and GELU.\n",
    "        \"\"\"\n",
    "        for expert in self.intermediate.experts:\n",
    "            # Copy weights and biases from the pretrained FFN\n",
    "            expert[0].weight.data.copy_(pretrained_ffn[0].weight.data.clone())\n",
    "            expert[0].bias.data.copy_(pretrained_ffn[0].bias.data.clone())\n",
    "            expert[2].weight.data.copy_(pretrained_ffn[2].weight.data.clone())\n",
    "            expert[2].bias.data.copy_(pretrained_ffn[2].bias.data.clone())\n",
    "\n",
    "            # Add small random noise to each expert to diversify them slightly\n",
    "            for param in expert.parameters():\n",
    "                param.data += 0.01 * torch.randn_like(param)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the MoE-enhanced BERT layer.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_states (Tensor): Input embeddings (batch_size, seq_len, hidden_dim)\n",
    "            attention_mask (Tensor, optional): Mask for attention (used for padding).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]:\n",
    "                - Final hidden states after attention + MoE + residual + norm\n",
    "                - Gating weights from MoE (for interpretability)\n",
    "        \"\"\"\n",
    "        # Standard BERT attention\n",
    "        attention_output = self.attention(hidden_states, attention_mask)[0]\n",
    "        attention_output = self.attention_output(attention_output, hidden_states)\n",
    "\n",
    "        # MoE block replaces the original FFN\n",
    "        moe_output, gate_weights = self.intermediate(attention_output)\n",
    "\n",
    "        # Final dense layer, dropout, and residual connection with layer norm\n",
    "        ffn_output = self.output_dense(moe_output)\n",
    "        ffn_output = self.output_dropout(ffn_output)\n",
    "        layer_output = self.output_norm(ffn_output + attention_output)\n",
    "\n",
    "        return layer_output, gate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4d0a0-f335-4777-ae04-9f9d1b9a91b0",
   "metadata": {},
   "source": [
    "### MoeBERTModel: Full BERT Encoder with MoE-Enhanced Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf48cd5-7704-487f-9956-d7ccfe5094d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeBERTModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Full BERT model where each encoder layer uses a Mixture-of-Experts (MoE) feed-forward module\n",
    "    instead of the standard FFN. Initialized from a pretrained BERT model to inherit knowledge.\n",
    "\n",
    "    Parameters:\n",
    "        config (BertConfig): BERT configuration object.\n",
    "        num_experts (int): Number of experts per layer.\n",
    "        top_k (int): Number of active experts selected by the gating mechanism per token.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_experts=7, top_k=4):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        # Token + position + segment embeddings\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "\n",
    "        # Replace original BERT encoder layers with MoE-enhanced layers\n",
    "        self.encoder_layers = torch.nn.ModuleList([\n",
    "            BertLayerWithMoE(config, num_experts=num_experts, top_k=top_k)\n",
    "            for hidden_layer in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        # Pooler: creates a single embedding from the [CLS] token\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.init_weights()\n",
    "\n",
    "        # Load a standard BERT model to transfer pretrained weights into the MoE experts\n",
    "        base_bert = BertModel(config)\n",
    "        base_bert.eval()  # We only use it to read weights\n",
    "\n",
    "        for i, moe_layer in enumerate(self.encoder_layers):\n",
    "            # Extract the original FFN components from the i-th BERT layer\n",
    "            intermediate = base_bert.encoder.layer[i].intermediate.dense\n",
    "            output = base_bert.encoder.layer[i].output.dense\n",
    "\n",
    "            # Rebuild the FFN module to match the MoE expert structure\n",
    "            pretrained_ffn = torch.nn.Sequential(\n",
    "                intermediate,\n",
    "                torch.nn.GELU(),\n",
    "                output\n",
    "            )\n",
    "\n",
    "            # Initialize each expert in the MoE layer with this pretrained FFN\n",
    "            moe_layer.initialize_experts_from_ffn(pretrained_ffn)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the full MoE-enhanced BERT model.\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (Tensor): Token IDs (batch_size, seq_len).\n",
    "            attention_mask (Tensor, optional): Mask to ignore padded tokens.\n",
    "            token_type_ids (Tensor, optional): Segment IDs (for sentence pairs).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, List[Tensor]]:\n",
    "                - Last hidden states for all tokens\n",
    "                - Pooled output (usually from [CLS] token)\n",
    "                - List of gating weights from all layers (for inspection/analysis)\n",
    "        \"\"\"\n",
    "        # Get embeddings for input tokens\n",
    "        embedding_output = self.embeddings(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        # Create attention mask in BERT's expected format\n",
    "        if attention_mask is not None:\n",
    "            extended_attention_mask = attention_mask[:, None, None, :]  # (batch, 1, 1, seq_len)\n",
    "            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0  # Masked positions get large negative\n",
    "        else:\n",
    "            extended_attention_mask = None\n",
    "\n",
    "        hidden_states = embedding_output\n",
    "        gate_weights_list = []  # To store MoE gate weights per layer\n",
    "\n",
    "        # Pass input through each encoder layer (with MoE)\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states, gate_weights = layer(hidden_states, attention_mask=extended_attention_mask)\n",
    "            gate_weights_list.append(gate_weights)\n",
    "\n",
    "        # Final pooled embedding (typically from [CLS] token)\n",
    "        pooled_output = self.pooler(hidden_states)\n",
    "\n",
    "        return hidden_states, pooled_output, gate_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34509590-9807-43ad-8ce5-417e0be59542",
   "metadata": {},
   "source": [
    "### MoeBERTScorer: Essay Scoring Head with Optional Handcrafted Features & MoE Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c72a2-be3d-4b92-acd3-0d0c4c0dc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeBERTScorer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A regression head built on top of MoeBERTModel to predict essay scores.\n",
    "    Supports optional handcrafted features and auxiliary loss for encouraging balanced expert usage.\n",
    "\n",
    "    Parameters:\n",
    "        base_model (MoeBERTModel): The base encoder model with MoE layers.\n",
    "        dropout (float): Dropout probability in the regression layer.\n",
    "        feature_dim (int): Number of external handcrafted features to concatenate with the [CLS] output.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: MoeBERTModel, dropout=0.2, feature_dim=0):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model                     # MoeBERT encoder\n",
    "        self.feature_dim = feature_dim                # Dimension of handcrafted features\n",
    "\n",
    "        # Total input dimension = [CLS] embedding + optional handcrafted features\n",
    "        input_dim = self.encoder.config.hidden_size + feature_dim\n",
    "\n",
    "        # Simple regressor: dense layer + dropout to predict a score\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 1),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n",
    "                features=None, labels=None, aux_loss_weight=0.5):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (Tensor): Token IDs.\n",
    "            attention_mask (Tensor, optional): Padding mask.\n",
    "            token_type_ids (Tensor, optional): Segment IDs.\n",
    "            features (Tensor, optional): Extra handcrafted features.\n",
    "            labels (Tensor, optional): Ground truth scores.\n",
    "            aux_loss_weight (float): Weight for entropy-based auxiliary loss on MoE gating.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary with:\n",
    "                - 'loss': Total loss (MSE + optional MoE entropy loss)\n",
    "                - 'logits': Predicted scores\n",
    "                - 'hidden_states': Final hidden layer outputs\n",
    "                - 'aux_loss': Entropy-based auxiliary regularization term (if labels are provided)\n",
    "        \"\"\"\n",
    "        # Encode input through MoE BERT\n",
    "        hidden_states, pooled_output, gate_weights_list = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        last_gate = gate_weights_list[-1]  # Use last layer's gate weights for auxiliary metrics\n",
    "\n",
    "        # If entropy regularization is enabled and shape is valid\n",
    "        if aux_loss_weight > 0 and last_gate.ndim == 3:\n",
    "            usage_mask = (last_gate > 0).float()  # Binary mask: which experts are used\n",
    "\n",
    "            usage_count = usage_mask.sum(dim=(0, 1))  # Count how often each expert is selected\n",
    "            self.expert_usage_counts = usage_count.detach().cpu()\n",
    "\n",
    "            # Calculate entropy of expert usage distribution\n",
    "            prob_dist = self.expert_usage_counts / self.expert_usage_counts.sum()\n",
    "            self.expert_entropy = -(prob_dist * torch.log(prob_dist + 1e-8)).sum().item()\n",
    "\n",
    "        # Save mean gate weights per expert (for diagnostics)\n",
    "        self.last_gate_weights = gate_weights_list[-1].mean(dim=1).detach().cpu()\n",
    "\n",
    "        # Optionally concatenate handcrafted features\n",
    "        if features is not None:\n",
    "            pooled_output = torch.cat([pooled_output, features], dim=-1)\n",
    "\n",
    "        # Predict final score\n",
    "        score = self.regressor(pooled_output).squeeze(-1)  # (batch_size,)\n",
    "\n",
    "        loss = None  # Initialize loss\n",
    "\n",
    "        if labels is not None:\n",
    "            # Base loss: mean squared error between predicted and true scores\n",
    "            loss = torch.nn.functional.mse_loss(score, labels.float())\n",
    "\n",
    "            if gate_weights_list:\n",
    "                gate_weights = gate_weights_list[-1]                # Use final layer's gates\n",
    "                mean_gates = gate_weights.mean(dim=(0, 1))          # Average over batch and sequence\n",
    "                entropy = -(mean_gates * torch.log(mean_gates + 1e-9)).sum()  # Entropy of expert usage\n",
    "                aux_loss = -entropy                                 # Encourage high entropy (diverse usage)\n",
    "\n",
    "                # Add auxiliary loss to total loss\n",
    "                loss = loss + aux_loss_weight * aux_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": score,\n",
    "            \"hidden_states\": hidden_states,\n",
    "            \"aux_loss\": aux_loss if labels is not None else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8975e-6a27-4ac9-a85b-098cababfb42",
   "metadata": {},
   "source": [
    "### Freeze BERT Layers Except the Last Few (Fine-Tuning Strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b6189-14c1-4642-8a8e-1abe5a03888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_bert_layers(model, num_unfrozen=2):\n",
    "    \"\"\"\n",
    "    Freezes most of the BERT model parameters to reduce training cost and avoid overfitting,\n",
    "    except for the last `num_unfrozen` encoder layers and the pooler.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): A BERT-based model (e.g., MoeBERTModel or BertModel).\n",
    "        num_unfrozen (int): Number of encoder layers (from the top) to keep trainable.\n",
    "    \"\"\"\n",
    "    # Freeze all parameters by default\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # If model has an encoder with layers, selectively unfreeze the last few layers\n",
    "    if hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):\n",
    "        total_layers = len(model.encoder.layer)\n",
    "\n",
    "        # Unfreeze only the last `num_unfrozen` layers\n",
    "        for i in range(total_layers - num_unfrozen, total_layers):\n",
    "            for param in model.encoder.layer[i].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Also unfreeze the pooler if present (e.g., to fine-tune [CLS] token representation)\n",
    "    if hasattr(model, \"pooler\"):  \n",
    "        for param in model.pooler.parameters():\n",
    "            param.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aes_env]",
   "language": "python",
   "name": "conda-env-aes_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
