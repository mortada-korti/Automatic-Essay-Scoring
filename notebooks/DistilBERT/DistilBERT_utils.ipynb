{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc92bf28-8b07-4256-a531-edebed23daab",
   "metadata": {},
   "source": [
    "# DistilBERT_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e219160-481d-41b9-8adc-c833e2ee9945",
   "metadata": {},
   "source": [
    "### Import Core DistilBERT Components and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05ac14-bf78-4cef-b902-c2c6c0cc4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert.modeling_distilbert import (\n",
    "    Embeddings,            # Token + position embeddings for DistilBERT\n",
    "    TransformerBlock       # Basic encoder block used in place of full-layer modules\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertModel,        # Pretrained DistilBERT backbone\n",
    "    DistilBertPreTrainedModel,  # Base class for custom heads\n",
    "    DistilBertConfig        # Configuration for model structure\n",
    ")\n",
    "\n",
    "import torch            \n",
    "import numpy as np         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444dac30-7e1e-42e2-854e-2ff5e904d1cb",
   "metadata": {},
   "source": [
    "### MoEFeedForward: Mixture-of-Experts Block for DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7152e-a221-4569-904a-194c89be14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEFeedForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Mixture-of-Experts (MoE) feed-forward module used to replace or enhance\n",
    "    the standard feed-forward layer in transformer blocks.\n",
    "\n",
    "    Each input token is routed to a subset of expert networks, either densely or\n",
    "    using top-k sparse gating.\n",
    "\n",
    "    Parameters:\n",
    "        hidden_dim (int): Input and output dimensionality.\n",
    "        intermediate_dim (int): Hidden size inside each expert’s MLP.\n",
    "        num_experts (int): Total number of expert networks.\n",
    "        dropout (float): Dropout probability after combining expert outputs.\n",
    "        top_k (int): Number of experts to activate per token.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, intermediate_dim, num_experts=7, dropout=0.2, top_k=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "\n",
    "        # Define each expert as a two-layer MLP with GELU activation\n",
    "        self.experts = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_dim, intermediate_dim),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(intermediate_dim, hidden_dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # Gating network to compute expert weights per token\n",
    "        self.gate = torch.nn.Linear(hidden_dim, num_experts)\n",
    "\n",
    "        # Dropout applied after expert aggregation\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute gating logits for all experts\n",
    "        gate_logits = self.gate(x)  # (B, T, E)\n",
    "\n",
    "        # Apply sparse top-k gating if configured\n",
    "        if self.top_k > 0 and self.top_k < self.num_experts:\n",
    "            topk_values, topk_indices = torch.topk(gate_logits, self.top_k, dim=-1)\n",
    "            mask = torch.full_like(gate_logits, float('-inf'))\n",
    "            mask.scatter_(-1, topk_indices, topk_values)\n",
    "            gate_weights = torch.nn.functional.softmax(mask, dim=-1)\n",
    "        else:\n",
    "            gate_weights = torch.nn.functional.softmax(gate_logits, dim=-1)  # Dense routing\n",
    "\n",
    "        # Compute each expert’s output\n",
    "        expert_outputs = [expert(x) for expert in self.experts]     # List of (B, T, H)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=2)         # Shape: (B, T, E, H)\n",
    "\n",
    "        # Apply gate weights and sum across experts\n",
    "        gate_weights = gate_weights.unsqueeze(-1)                   # (B, T, E, 1)\n",
    "        weighted_output = expert_outputs * gate_weights             # (B, T, E, H)\n",
    "        output = weighted_output.sum(dim=2)                         # (B, T, H)\n",
    "\n",
    "        return self.dropout(output), gate_weights.squeeze(-1)      # Output + gate weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bac4f4-871b-4b4f-bc88-1da60d6f43bc",
   "metadata": {},
   "source": [
    "### DistilBertLayerWithMoE: MoE-Integrated Encoder Block for DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb663db-aba0-4434-bc3b-2bf2125ec0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertLayerWithMoE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer block for DistilBERT with a Mixture-of-Experts (MoE) feed-forward layer.\n",
    "\n",
    "    This replaces the standard FFN with a dynamic expert-routing mechanism while preserving\n",
    "    DistilBERT’s simplified architecture and pre-normalization scheme.\n",
    "\n",
    "    Parameters:\n",
    "        config (DistilBertConfig): Model configuration.\n",
    "        num_experts (int): Number of experts per MoE layer.\n",
    "        top_k (int): Number of experts to select per token (sparse gating).\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_experts=7, top_k=2):\n",
    "        super().__init__()\n",
    "\n",
    "        from transformers.models.distilbert.modeling_distilbert import MultiHeadSelfAttention\n",
    "\n",
    "        # Attention sub-layer\n",
    "        self.dim = config.dim\n",
    "        self.hidden_dropout = config.dropout\n",
    "        self.attention = MultiHeadSelfAttention(config)\n",
    "        self.attention_dropout = torch.nn.Dropout(config.dropout)\n",
    "        self.attention_norm = torch.nn.LayerNorm(config.dim, eps=1e-12)\n",
    "\n",
    "        # Mixture-of-Experts as the FFN replacement\n",
    "        self.intermediate = MoEFeedForward(\n",
    "            hidden_dim=config.dim,\n",
    "            intermediate_dim=config.hidden_dim,\n",
    "            num_experts=num_experts,\n",
    "            dropout=config.dropout,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        self.output_dropout = torch.nn.Dropout(config.dropout)\n",
    "        self.output_norm = torch.nn.LayerNorm(config.dim, eps=1e-12)\n",
    "\n",
    "    def initialize_experts_from_ffn(self, pretrained_ffn):\n",
    "        \"\"\"\n",
    "        Copy weights from a pretrained FFN into all MoE experts.\n",
    "        Adds small Gaussian noise to diversify initialization.\n",
    "        \"\"\"\n",
    "        for expert in self.intermediate.experts:\n",
    "            expert[0].weight.data.copy_(pretrained_ffn[0].weight.data.clone())\n",
    "            expert[0].bias.data.copy_(pretrained_ffn[0].bias.data.clone())\n",
    "            expert[2].weight.data.copy_(pretrained_ffn[2].weight.data.clone())\n",
    "            expert[2].bias.data.copy_(pretrained_ffn[2].bias.data.clone())\n",
    "\n",
    "            for param in expert.parameters():\n",
    "                param.data += 0.01 * torch.randn_like(param)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        # Pre-norm → self-attention → dropout → residual\n",
    "        normed_hidden = self.attention_norm(hidden_states)\n",
    "        attn_output = self.attention(\n",
    "            normed_hidden, normed_hidden, normed_hidden,\n",
    "            attention_mask, head_mask=head_mask\n",
    "        )[0]\n",
    "        attn_output = self.attention_dropout(attn_output)\n",
    "        attn_output = attn_output + hidden_states\n",
    "\n",
    "        # Pre-norm → MoE FFN → dropout → residual\n",
    "        normed_attn_output = self.output_norm(attn_output)\n",
    "        ffn_output, gate_weights = self.intermediate(normed_attn_output)\n",
    "        ffn_output = self.output_dropout(ffn_output)\n",
    "        ffn_output = ffn_output + attn_output\n",
    "\n",
    "        return ffn_output, gate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b4682-9876-4abc-81ff-1148229af47f",
   "metadata": {},
   "source": [
    "### MoeDistilBertModel: Lightweight DistilBERT with Mixture-of-Experts Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf48cd5-7704-487f-9956-d7ccfe5094d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeDistilBertModel(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, num_experts=7, top_k=4):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        from transformers.models.distilbert.modeling_distilbert import Embeddings\n",
    "\n",
    "        self.embeddings = Embeddings(config)\n",
    "\n",
    "        self.encoder_layers = torch.nn.ModuleList([\n",
    "            DistilBertLayerWithMoE(config, num_experts=num_experts, top_k=top_k)\n",
    "            for _ in range(config.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        base_model = DistilBertModel(config)\n",
    "        base_model.eval()\n",
    "\n",
    "        for i, moe_layer in enumerate(self.encoder_layers):\n",
    "            ffn1 = base_model.transformer.layer[i].ffn.lin1\n",
    "            ffn2 = base_model.transformer.layer[i].ffn.lin2\n",
    "\n",
    "            pretrained_ffn = torch.nn.Sequential(\n",
    "                ffn1,\n",
    "                torch.nn.GELU(),\n",
    "                ffn2\n",
    "            )\n",
    "\n",
    "            moe_layer.initialize_experts_from_ffn(pretrained_ffn)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedding_output = self.embeddings(input_ids=input_ids)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            extended_attention_mask = attention_mask[:, None, :]\n",
    "        else:\n",
    "            extended_attention_mask = None\n",
    "\n",
    "        hidden_states = embedding_output\n",
    "        gate_weights_list = []\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states, gate_weights = layer(hidden_states, attention_mask=extended_attention_mask)\n",
    "            gate_weights_list.append(gate_weights)\n",
    "\n",
    "        pooled_output = hidden_states[:, 0]\n",
    "\n",
    "        return hidden_states, pooled_output, gate_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583cf8a-fb00-4988-92c5-a5294ee5e47f",
   "metadata": {},
   "source": [
    "### MoeDistilBertScorer: Regression Head with Expert Usage & Entropy Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c72a2-be3d-4b92-acd3-0d0c4c0dc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeDistilBertScorer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A regression head for MoeDistilBertModel that:\n",
    "    - Supports optional handcrafted feature fusion\n",
    "    - Computes auxiliary entropy loss for expert diversity\n",
    "    - Tracks expert usage for diagnostics\n",
    "\n",
    "    Parameters:\n",
    "        base_model (MoeDistilBertModel): MoE-augmented DistilBERT encoder.\n",
    "        dropout (float): Dropout rate after pooling.\n",
    "        feature_dim (int): Dimensionality of optional handcrafted features.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: MoeDistilBertModel, dropout=0.2, feature_dim=0):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        input_dim = self.encoder.config.dim + feature_dim  # DistilBERT uses `dim` instead of `hidden_size`\n",
    "\n",
    "        # Output head for regression\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 1),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, features=None, labels=None, aux_loss_weight=0.5):\n",
    "        # Run input through encoder\n",
    "        hidden_states, pooled_output, gate_weights_list = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # Analyze last layer gate weights\n",
    "        last_gate = gate_weights_list[-1]\n",
    "        if aux_loss_weight > 0 and last_gate.ndim == 3:\n",
    "            usage_mask = (last_gate > 0).float()\n",
    "            usage_count = usage_mask.sum(dim=(0, 1))\n",
    "            self.expert_usage_counts = usage_count.detach().cpu()\n",
    "\n",
    "            prob_dist = self.expert_usage_counts / self.expert_usage_counts.sum()\n",
    "            self.expert_entropy = -(prob_dist * torch.log(prob_dist + 1e-8)).sum().item()\n",
    "\n",
    "        self.last_gate_weights = gate_weights_list[-1].mean(dim=1).detach().cpu()\n",
    "\n",
    "        # Concatenate handcrafted features if provided\n",
    "        if features is not None:\n",
    "            pooled_output = torch.cat([pooled_output, features], dim=-1)\n",
    "\n",
    "        # Predict scalar score\n",
    "        score = self.regressor(pooled_output).squeeze(-1)\n",
    "\n",
    "        # Compute losses if labels are provided\n",
    "        loss = None\n",
    "        aux_loss = None\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.functional.mse_loss(score, labels.float())\n",
    "\n",
    "            if gate_weights_list:\n",
    "                gate_weights = gate_weights_list[-1]\n",
    "                mean_gates = gate_weights.mean(dim=(0, 1))\n",
    "                entropy = -(mean_gates * torch.log(mean_gates + 1e-9)).sum()\n",
    "                aux_loss = -entropy\n",
    "                loss = loss + aux_loss_weight * aux_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": score,\n",
    "            \"hidden_states\": hidden_states,\n",
    "            \"aux_loss\": aux_loss if labels is not None else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5adc3f3-7e9a-41b5-ac68-04f5b424c3a9",
   "metadata": {},
   "source": [
    "### freeze_distilbert_layers: Selective Layer Unfreezing for Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b6189-14c1-4642-8a8e-1abe5a03888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_distilbert_layers(model, num_unfrozen=2):\n",
    "    \"\"\"\n",
    "    Freezes all layers of a DistilBERT-based model except the last `num_unfrozen` encoder layers.\n",
    "\n",
    "    Useful for:\n",
    "    - Reducing training time and memory\n",
    "    - Retaining general-purpose language features in early layers\n",
    "    - Preventing overfitting on small downstream tasks\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): A MoeDistilBertModel or compatible DistilBERT-based model.\n",
    "        num_unfrozen (int): Number of top encoder layers to keep trainable.\n",
    "    \"\"\"\n",
    "    if num_unfrozen is not None: \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False  # Freeze entire model by default\n",
    "    \n",
    "        # Unfreeze only the last `num_unfrozen` encoder layers\n",
    "        if hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):\n",
    "            total_layers = len(model.encoder.layer)\n",
    "            for i in range(total_layers - num_unfrozen, total_layers):\n",
    "                for param in model.encoder.layer[i].parameters():\n",
    "                    param.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aes_env]",
   "language": "python",
   "name": "conda-env-aes_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
