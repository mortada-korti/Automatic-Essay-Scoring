{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31aa1b9-9233-4ec2-bebb-0d8bde7c5fb2",
   "metadata": {},
   "source": [
    "# Longformer_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503f148-240a-478b-8ff4-1a1823f3951c",
   "metadata": {},
   "source": [
    "### Import Training Utilities, Metrics, and Setup Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829f1f1-27c8-43f5-8d24-76c643699f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, LongformerTokenizer, LongformerConfig, Trainer\n",
    "from sklearn.metrics import cohen_kappa_score \n",
    "from datasets import Dataset  \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import warnings  \n",
    "import sys, os  \n",
    "\n",
    "# Hide warnings to keep notebook output clean\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46882ba-550b-4792-9fe8-3140c62e7fea",
   "metadata": {},
   "source": [
    "### Environment Flags to Ensure Stable Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17517aab-2fe1-4492-a9b7-c821c2b079f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Prevents parallel tokenizer threads — avoids potential race conditions or console spam\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"       # Disables TorchInductor (experimental compiler backend) to ensure compatibility\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"       # Disables PyTorch 2.0's torch.compile functionality (can cause issues in custom models)\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"         # Turns off TorchDynamo, another dynamic optimization engine in PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b780d7c-81b5-4e93-87b8-0e0cc0d78aec",
   "metadata": {},
   "source": [
    "### Set Project Root Directory and Add to Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c07771-9185-451f-8f78-7be51abd6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))  # Define the root directory two levels above the current working directory\n",
    "sys.path.append(ROOT_DIR)   # Add the root directory to Python's module search path so that custom modules can be imported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c786e-2fa9-4795-8a3e-4e9a428359ec",
   "metadata": {},
   "source": [
    "### Import Core Project Modules and Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce86c80-c36e-4b5f-87c3-2afe1409b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "\n",
    "# Project-specific configuration: paths for data and output storage\n",
    "from config import DATA_DIR, RESULTS_DIR\n",
    "\n",
    "# Import MoE and Longformer model components from custom Longformer_utils script\n",
    "from scripts.Longformer.Longformer_utils import (\n",
    "    freeze_longformer_layers,\n",
    "    LongformerLayerWithMoE,\n",
    "    MoeLongformerScorer,\n",
    "    MoeLongformerModel,\n",
    "    MoEFeedForward,  \n",
    ")\n",
    "\n",
    "# Import utility functions for data processing and scoring\n",
    "from scripts.utils import generate_train7_test1_splits, denormalize_score, preprocess  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f06cc0-09b4-4d6d-a0af-08c247be6fc9",
   "metadata": {},
   "source": [
    "### Load Essay Dataset from TSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a469e9-04ca-4b04-a279-83a364a66d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the essay dataset as a pandas DataFrame.\n",
    "df = pd.read_csv(f\"{DATA_DIR}/dataset.tsv\", delimiter=\"\\t\", encoding='ISO-8859-1')  # Load the full essay dataset as a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b7322-1239-42a1-bbf4-da737bb41dbc",
   "metadata": {},
   "source": [
    "### Set Device for Computation (GPU if Available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc941f-0112-4b5a-98fb-d0d2c9200b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU (CUDA) if available; otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "# Print how many CUDA-enabled GPUs are detected\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Show the index and name of the active GPU (if one is available)\n",
    "print(f\"Current device: {torch.cuda.current_device()} - {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a7d896-b1ed-4fd3-b1b9-5a7d09a1fe32",
   "metadata": {},
   "source": [
    "### Define Hyperparameter Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b0685-acb8-4eae-b32d-d979836b2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space = {\n",
    "    \"learning_rate\": [5e-5, 3e-5],     # Learning rates to try\n",
    "    \"batch_size\": [8],                 # Batch size (fixed)\n",
    "    \"epochs\": [15],                    # Number of training epochs\n",
    "    \"dropout\": [0.2],                  # Dropout rate for regularization\n",
    "    \"num_experts\": [7],                # Number of experts in each MoE layer\n",
    "    \"aux_loss_weight\": [0.5],          # Weight for auxiliary loss (entropy regularization)\n",
    "    \"unfrozen_layers\": [2],            # How many top Longformer layers to fine-tune\n",
    "    \"top_k\": [2]                       # Number of experts selected per token\n",
    "}\n",
    "\n",
    "# Generate all possible combinations of hyperparameters\n",
    "config_list = list(itertools.product(*config_space.values()))\n",
    "\n",
    "# Store the corresponding keys to map each config tuple\n",
    "config_keys = list(config_space.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24fd40-c858-48cd-a6f9-85ff527152c3",
   "metadata": {},
   "source": [
    "### Load Existing Results (If Any) to Avoid Redundant Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890662c-e997-42ef-b1da-bc04823180fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path(f\"{RESULTS_DIR}/Longformer/Longformer_results.csv\")  # Path to CSV where previous results are saved\n",
    "\n",
    "if results_path.exists():\n",
    "    existing_df = pd.read_csv(results_path)  # Load previously saved results\n",
    "    existing_configs = existing_df[config_keys].to_dict(\"records\")  # Extract existing configurations to check for duplicates\n",
    "else:\n",
    "    existing_df = None                     # No existing results file found\n",
    "    existing_configs = []                  # Start fresh with an empty config list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33958fa-e292-4f89-8226-7332711bcc53",
   "metadata": {},
   "source": [
    "### Run Cross-Prompt MoE-Longformer Experiments Over All Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29374a-7255-4521-916a-c6c1e1590649",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = generate_train7_test1_splits()  # Generate 8-fold cross-prompt train/test splits\n",
    "results = []                             # Store results for each configuration\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")  # Load Longformer tokenizer\n",
    "\n",
    "for config_id, values in enumerate(config_list):\n",
    "    config = dict(zip(config_keys, values))  # Map hyperparameter values to names\n",
    "\n",
    "    # Skip this config if results already exist (for resume-safe training)\n",
    "    if config in existing_configs:\n",
    "        print(f\"*** Skipping Config {config_id+1}/{len(config_list)} — already completed ***\")\n",
    "        continue\n",
    "\n",
    "    print(\"\\n\", \"-\"*135)\n",
    "    print(f\"\\nRunning Config {config_id+1}/{len(config_list)}: {config}\")\n",
    "    print(\"-\"*135, \"\\n\")\n",
    "\n",
    "    test_qwks = []  # Collect QWK scores from each prompt fold\n",
    "\n",
    "    for split in splits:\n",
    "        # Split data by prompt\n",
    "        train_df = df[df[\"essay_set\"].isin(split[\"train\"])].copy()\n",
    "        test_df = df[df[\"essay_set\"] == split[\"test\"]].copy()\n",
    "\n",
    "        # Convert to Hugging Face Datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "        # Apply preprocessing (tokenization + handcrafted features)\n",
    "        train_dataset = train_dataset.map(lambda example: preprocess(example, tokenizer))\n",
    "        test_dataset = test_dataset.map(lambda example: preprocess(example, tokenizer))\n",
    "\n",
    "        n_handcrafted_features = len(train_dataset[0][\"features\"])  # Dimensionality of external features\n",
    "\n",
    "        # Load Longformer config and initialize MoeLongformer model with MoE layers\n",
    "        longformer_config = LongformerConfig.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "        moe_model = MoeLongformerModel(longformer_config, num_experts=config[\"num_experts\"], top_k=config[\"top_k\"])\n",
    "        model = MoeLongformerScorer(base_model=moe_model, dropout=config[\"dropout\"], feature_dim=n_handcrafted_features)\n",
    "        model.to(device)\n",
    "\n",
    "        # Freeze lower layers of Longformer\n",
    "        freeze_longformer_config_layers(model.encoder, num_unfrozen=config[\"unfrozen_layers\"])\n",
    "\n",
    "       # Assign auxiliary loss weight for entropy-based expert regularization\n",
    "        model.encoder.config.aux_loss_weight = config[\"aux_loss_weight\"]\n",
    "\n",
    "        # Define training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=config[\"batch_size\"],\n",
    "            per_device_eval_batch_size=config[\"batch_size\"],\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            num_train_epochs=config[\"epochs\"],\n",
    "            eval_strategy=\"no\",       # No validation during training\n",
    "            save_strategy=\"no\",       # Do not save checkpoints\n",
    "            logging_strategy=\"no\",    # Suppress logs\n",
    "            report_to=\"none\",         # Disable external logging (e.g., WandB)\n",
    "            weight_decay=0.01,        # L2 regularization\n",
    "            warmup_ratio=0.1,         # Warmup for learning rate scheduler\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer with model and dataset\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Run prediction on test set\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "        preds = predictions.predictions[0]\n",
    "        labels = predictions.label_ids\n",
    "\n",
    "        # Convert predictions and labels back to original scoring scale\n",
    "        preds = denormalize_score(preds, test_df)\n",
    "        labels = denormalize_score(labels, test_df)\n",
    "\n",
    "        # Get average expert routing weights\n",
    "        gate_weights = model.last_gate_weights.numpy()\n",
    "        avg_gate_weights = gate_weights.mean(axis=0)\n",
    "\n",
    "        # Evaluate with Quadratic Weighted Kappa\n",
    "        qwk = cohen_kappa_score(labels, preds, weights=\"quadratic\")\n",
    "        test_qwks.append(qwk)\n",
    "\n",
    "        print(\"\\n\", \"-\"*50)\n",
    "        print(f\"Config {config_id+1}/{len(config_list)} -> prompt_{split['test']} -> Test QWK: {qwk:.4f}\")\n",
    "        print(\"-\"*50, \"\\n\")\n",
    "\n",
    "    # Average QWK across all prompts\n",
    "    avg_qwk = sum(test_qwks) / len(test_qwks)\n",
    "\n",
    "    # Prepare result row with metrics and expert routing stats\n",
    "    result_row = {\n",
    "        **config,\n",
    "        **{f\"prompt_{i+1}\": q for i, q in enumerate(test_qwks)},\n",
    "        \"avg_qwk\": avg_qwk,\n",
    "        **{f\"pi_expert_{i}\": avg_gate_weights[i] for i in range(len(avg_gate_weights))}\n",
    "    }\n",
    "\n",
    "    results.append(result_row)  # Save result to memory\n",
    "\n",
    "    # Save running results to disk after each config\n",
    "    pd.DataFrame(results).to_csv(f\"{RESULTS_DIR}/Longformer/Longformer_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aes_env]",
   "language": "python",
   "name": "conda-env-aes_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
