{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1647d13e-ff33-440d-9e00-5fb13709fd43",
   "metadata": {},
   "source": [
    "# Longformer_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f66176-764f-4920-9927-c40fcf2b7696",
   "metadata": {},
   "source": [
    "### Import Core Longformer Components and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05ac14-bf78-4cef-b902-c2c6c0cc4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.longformer.modeling_longformer import (\n",
    "    LongformerSelfAttention,\n",
    "    LongformerSelfOutput,\n",
    "    LongformerEmbeddings,\n",
    "    LongformerPooler\n",
    ")\n",
    "from transformers import LongformerModel, LongformerPreTrainedModel\n",
    "\n",
    "import numpy as np\n",
    "import torch    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50d7c7-10c6-4749-8cdd-dc99f2592a70",
   "metadata": {},
   "source": [
    "### Mixture-of-Experts Feed-Forward Layer for Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7152e-a221-4569-904a-194c89be14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEFeedForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Mixture-of-Experts (MoE) feed-forward layer that replaces the \n",
    "    standard FFN in Longformer. Multiple expert networks process the input, and a gating \n",
    "    mechanism determines how to combine their outputs.\n",
    "\n",
    "    Parameters:\n",
    "        hidden_dim (int): Input and output dimension (usually the Longformer hidden size).\n",
    "        intermediate_dim (int): Hidden size within each expert's feed-forward network.\n",
    "        num_experts (int): Total number of parallel expert networks.\n",
    "        dropout (float): Dropout probability applied to the final output.\n",
    "        top_k (int): Number of top experts to select (sparse gating); if set to 0, use all experts.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, intermediate_dim, num_experts=7, dropout=0.2, top_k=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k  \n",
    "\n",
    "        # Define the expert networks — each one is a small feed-forward block\n",
    "        self.experts = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_dim, intermediate_dim),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(intermediate_dim, hidden_dim)\n",
    "            ) for expert in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # Gating network: assigns weights to each expert based on input\n",
    "        self.gate = torch.nn.Linear(hidden_dim, num_experts)\n",
    "\n",
    "        # Dropout applied to final output\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MoE layer.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: \n",
    "                - Output tensor of the same shape as input after combining expert outputs.\n",
    "                - Gating weights tensor (used for interpretability or diagnostics).\n",
    "        \"\"\"\n",
    "        gate_logits = self.gate(x)  # Compute unnormalized scores for each expert\n",
    "\n",
    "        # Use top-k gating to select a few experts per token\n",
    "        if self.top_k > 0 and self.top_k < self.num_experts:\n",
    "            topk_values, topk_indices = torch.topk(gate_logits, self.top_k, dim=-1)  # (batch, seq_len, top_k)\n",
    "\n",
    "            # Create a full mask and fill in top-k positions with their logits\n",
    "            mask = torch.full_like(gate_logits, float('-inf')) \n",
    "            mask.scatter_(-1, topk_indices, topk_values)\n",
    "\n",
    "            # Apply softmax to get normalized gating weights (only top-k will be nonzero)\n",
    "            gate_weights = torch.nn.functional.softmax(mask, dim=-1)\n",
    "        else:\n",
    "            # If top_k is 0 or equal to number of experts, use all experts\n",
    "            gate_weights = torch.nn.functional.softmax(gate_logits, dim=-1)\n",
    "\n",
    "        # Compute outputs from each expert\n",
    "        expert_outputs = [expert(x) for expert in self.experts]  # List of (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # Stack expert outputs: shape becomes (batch, seq_len, num_experts, hidden_dim)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=2)\n",
    "\n",
    "        # Reshape gate weights to align for broadcasting: (batch, seq_len, num_experts, 1)\n",
    "        gate_weights = gate_weights.unsqueeze(-1)\n",
    "\n",
    "        # Multiply each expert's output by its corresponding gate weight\n",
    "        weighted_output = expert_outputs * gate_weights  # (batch, seq_len, num_experts, hidden_dim)\n",
    "\n",
    "        # Sum over experts to get final output per token\n",
    "        output = weighted_output.sum(dim=2)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        return self.dropout(output), gate_weights.squeeze(-1)  # Also return gate weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff4cba-78f2-444f-a649-6eea73dbde14",
   "metadata": {},
   "source": [
    "### Custom Longformer Layer with Mixture-of-Experts (MoE) Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb663db-aba0-4434-bc3b-2bf2125ec0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongformerLayerWithMoE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A custom Longformer layer that replaces the standard feed-forward sublayer with a \n",
    "    Mixture-of-Experts (MoE) module. It keeps the original attention mechanism intact.\n",
    "    \n",
    "    Parameters:\n",
    "        config (LongformerConfig): Configuration object for Longformer.\n",
    "        num_experts (int): Number of expert FFNs in the MoE block.\n",
    "        top_k (int): Number of experts to activate per token (sparse routing).\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_experts=7, top_k=2):\n",
    "        super().__init__()\n",
    "        self.attention = LongformerSelfAttention(config, layer_id=0)\n",
    "        self.attention_output = LongformerSelfOutput(config)\n",
    "\n",
    "        self.intermediate = MoEFeedForward(\n",
    "            num_experts=num_experts,\n",
    "            hidden_dim=config.hidden_size,\n",
    "            intermediate_dim=config.intermediate_size,\n",
    "            dropout=config.hidden_dropout_prob,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        self.output_dense = torch.nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.output_dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.output_norm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def initialize_experts_from_ffn(self, pretrained_ffn):\n",
    "        \"\"\"\n",
    "        Initialize all experts in the MoE block by copying weights from a standard \n",
    "        pretrained FFN (e.g., from a vanilla Longformer model). Adds small noise to break symmetry.\n",
    "\n",
    "        Parameters:\n",
    "            pretrained_ffn (torch.nn.Sequential): A standard FFN block with 2 Linear layers and GELU.\n",
    "        \"\"\"\n",
    "        for expert in self.intermediate.experts:\n",
    "            # Copy weights and biases from the pretrained FFN\n",
    "            expert[0].weight.data.copy_(pretrained_ffn[0].weight.data.clone())\n",
    "            expert[0].bias.data.copy_(pretrained_ffn[0].bias.data.clone())\n",
    "            expert[2].weight.data.copy_(pretrained_ffn[2].weight.data.clone())\n",
    "            expert[2].bias.data.copy_(pretrained_ffn[2].bias.data.clone())\n",
    "\n",
    "            # Add small random noise to diversify experts\n",
    "            for param in expert.parameters():\n",
    "                param.data += 0.01 * torch.randn_like(param)\n",
    "                \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "        is_index_masked=None,\n",
    "        is_global_attn=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass through Longformer layer with MoE-enhanced FFN.\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: (batch_size, seq_len, hidden_dim)\n",
    "            attention_mask: Attention mask\n",
    "            output_attentions: Whether to return attention weights\n",
    "            is_index_masked: Masked positions\n",
    "            is_global_attn: Global attention positions\n",
    "            \n",
    "        Returns:\n",
    "            (layer_output, gate_weights)\n",
    "        \"\"\"\n",
    "        attention_output = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            is_index_masked=is_index_masked,\n",
    "            is_global_attn=is_global_attn\n",
    "        )[0]\n",
    "\n",
    "        attention_output = self.attention_output(attention_output, hidden_states)\n",
    "\n",
    "        moe_output, gate_weights = self.intermediate(attention_output)\n",
    "        ffn_output = self.output_dense(moe_output)\n",
    "        ffn_output = self.output_dropout(ffn_output)\n",
    "        layer_output = self.output_norm(ffn_output + attention_output)\n",
    "\n",
    "        return layer_output, gate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4d0a0-f335-4777-ae04-9f9d1b9a91b0",
   "metadata": {},
   "source": [
    "### MoeLongformerModel: Full Longformer Encoder with MoE-Enhanced Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf48cd5-7704-487f-9956-d7ccfe5094d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeLongformerModel(LongformerPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Longformer model with Mixture-of-Experts (MoE) feed-forward blocks in each encoder layer.\n",
    "\n",
    "    Parameters:\n",
    "        config (LongformerConfig): Longformer configuration object.\n",
    "        num_experts (int): Number of experts per layer.\n",
    "        top_k (int): Number of active experts selected per token.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_experts=7, top_k=4):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        # Token + position + type embeddings\n",
    "        self.embeddings = LongformerEmbeddings(config)\n",
    "\n",
    "        # Stack of encoder layers with MoE FFNs\n",
    "        self.encoder_layers = torch.nn.ModuleList([\n",
    "            LongformerLayerWithMoE(config, num_experts=num_experts, top_k=top_k)\n",
    "            for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        # Pooler (optional)\n",
    "        self.pooler = LongformerPooler(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        # Load from pretrained Longformer weights to initialize experts\n",
    "        base_model = LongformerModel(config)\n",
    "        base_model.eval()\n",
    "\n",
    "        for i, moe_layer in enumerate(self.encoder_layers):\n",
    "            intermediate = base_model.encoder.layer[i].intermediate.dense\n",
    "            output = base_model.encoder.layer[i].output.dense\n",
    "\n",
    "            pretrained_ffn = torch.nn.Sequential(\n",
    "                intermediate,\n",
    "                torch.nn.GELU(),\n",
    "                output\n",
    "            )\n",
    "\n",
    "            moe_layer.initialize_experts_from_ffn(pretrained_ffn)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, global_attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the MoE-enhanced Longformer.\n",
    "\n",
    "        Args:\n",
    "            input_ids: (batch, seq_len)\n",
    "            attention_mask: (batch, seq_len) → 1 for tokens to attend, 0 for padding\n",
    "            global_attention_mask: (batch, seq_len) → 1 for global attention tokens\n",
    "\n",
    "        Returns:\n",
    "            hidden_states, pooled_output, gate_weights_list\n",
    "        \"\"\"\n",
    "        embedding_output = self.embeddings(input_ids=input_ids)\n",
    "\n",
    "        hidden_states = embedding_output\n",
    "        gate_weights_list = []\n",
    "\n",
    "        # Iterate through each MoE encoder layer\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states, gate_weights = layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                is_index_masked=(attention_mask == 0 if attention_mask is not None else None),\n",
    "                is_global_attn=global_attention_mask\n",
    "            )\n",
    "            gate_weights_list.append(gate_weights)\n",
    "\n",
    "        # Apply pooling (e.g., [CLS] token or average pooling)\n",
    "        pooled_output = self.pooler(hidden_states)\n",
    "\n",
    "        return hidden_states, pooled_output, gate_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34509590-9807-43ad-8ce5-417e0be59542",
   "metadata": {},
   "source": [
    "### MoeLongformerScorer: Essay Scoring Head with Optional Handcrafted Features & MoE Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c72a2-be3d-4b92-acd3-0d0c4c0dc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeLongformerScorer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A regression head built on top of MoeLongformerModel to predict essay scores.\n",
    "    Supports optional handcrafted features and auxiliary loss for encouraging balanced expert usage.\n",
    "\n",
    "    Parameters:\n",
    "        base_model (MoeLongformerModel): The Longformer encoder model with MoE layers.\n",
    "        dropout (float): Dropout probability in the regression layer.\n",
    "        feature_dim (int): Number of external handcrafted features to concatenate with the [CLS] output.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, dropout=0.2, feature_dim=0):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        input_dim = self.encoder.config.hidden_size + feature_dim\n",
    "\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 1),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None,\n",
    "                features=None, labels=None, aux_loss_weight=0.5):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (Tensor): Token IDs.\n",
    "            attention_mask (Tensor, optional): Attention mask.\n",
    "            features (Tensor, optional): Handcrafted feature vector.\n",
    "            labels (Tensor, optional): Ground truth scores.\n",
    "            aux_loss_weight (float): Entropy regularization weight.\n",
    "\n",
    "        Returns:\n",
    "            dict with keys: loss, logits, hidden_states, aux_loss\n",
    "        \"\"\"\n",
    "        hidden_states, pooled_output, gate_weights_list = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        last_gate = gate_weights_list[-1]\n",
    "\n",
    "        if aux_loss_weight > 0 and last_gate.ndim == 3:\n",
    "            usage_mask = (last_gate > 0).float()\n",
    "            usage_count = usage_mask.sum(dim=(0, 1))\n",
    "            self.expert_usage_counts = usage_count.detach().cpu()\n",
    "\n",
    "            prob_dist = self.expert_usage_counts / self.expert_usage_counts.sum()\n",
    "            self.expert_entropy = -(prob_dist * torch.log(prob_dist + 1e-8)).sum().item()\n",
    "\n",
    "        self.last_gate_weights = last_gate.mean(dim=1).detach().cpu()\n",
    "\n",
    "        if features is not None:\n",
    "            pooled_output = torch.cat([pooled_output, features], dim=-1)\n",
    "\n",
    "        score = self.regressor(pooled_output).squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.functional.mse_loss(score, labels.float())\n",
    "\n",
    "            if gate_weights_list:\n",
    "                gate_weights = gate_weights_list[-1]\n",
    "                mean_gates = gate_weights.mean(dim=(0, 1))\n",
    "                entropy = -(mean_gates * torch.log(mean_gates + 1e-9)).sum()\n",
    "                aux_loss = -entropy\n",
    "                loss = loss + aux_loss_weight * aux_loss\n",
    "        else:\n",
    "            aux_loss = None\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": score,\n",
    "            \"hidden_states\": hidden_states,\n",
    "            \"aux_loss\": aux_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8975e-6a27-4ac9-a85b-098cababfb42",
   "metadata": {},
   "source": [
    "### Freeze Longformer Layers Except the Last Few (Fine-Tuning Strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b6189-14c1-4642-8a8e-1abe5a03888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_longformer_layers(model, num_unfrozen=2):\n",
    "    \"\"\"\n",
    "    Freezes most of the Longformer model parameters to reduce training cost and avoid overfitting,\n",
    "    except for the last `num_unfrozen` encoder layers and the pooler.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): A Longformer-based model (e.g., MoeLongformerModel or LongformerModel).\n",
    "        num_unfrozen (int): Number of encoder layers (from the top) to keep trainable.\n",
    "    \"\"\"\n",
    "    # Freeze all parameters initially\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze last few encoder layers\n",
    "    if hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):\n",
    "        total_layers = len(model.encoder.layer)\n",
    "        for i in range(total_layers - num_unfrozen, total_layers):\n",
    "            for param in model.encoder.layer[i].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Optional: Unfreeze embeddings if you want to fine-tune lower representations\n",
    "    if hasattr(model, \"embeddings\"):\n",
    "        for param in model.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Optional: Unfreeze pooler if model has one (usually it does)\n",
    "    if hasattr(model, \"pooler\"):\n",
    "        for param in model.pooler.parameters():\n",
    "            param.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aes_env]",
   "language": "python",
   "name": "conda-env-aes_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
