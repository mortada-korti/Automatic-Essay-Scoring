{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77de6351-ae2f-477d-94e7-77bc8205c919",
   "metadata": {},
   "source": [
    "# Longformer_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235fe0ca-f406-4684-a2d8-bebc079ca225",
   "metadata": {},
   "source": [
    "### Import Core Longformer Components and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05ac14-bf78-4cef-b902-c2c6c0cc4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.longformer.modeling_longformer import (\n",
    "    LongformerSelfAttention,      # Longformer's sliding-window self-attention mechanism\n",
    "    LongformerSelfOutput,         # Output transformation after self-attention\n",
    "    LongformerEmbeddings          # Embedding layer (tokens + position + segment)\n",
    ")\n",
    "from transformers import LongformerModel, LongformerPreTrainedModel  # Full Longformer model and base class for custom subclasses  \n",
    "import numpy as np      \n",
    "import torch            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088e3c2-1273-458f-924d-d5dd2e0f1604",
   "metadata": {},
   "source": [
    "### MoEFeedForward: Mixture-of-Experts FFN for Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7152e-a221-4569-904a-194c89be14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEFeedForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Mixture-of-Experts (MoE) feed-forward layer with top-k sparse gating.\n",
    "\n",
    "    Parameters:\n",
    "        hidden_dim (int): Dimensionality of the input and output.\n",
    "        intermediate_dim (int): Size of each expert's hidden layer.\n",
    "        num_experts (int): Total number of expert networks.\n",
    "        dropout (float): Dropout probability applied after combining expert outputs.\n",
    "        top_k (int): Number of top experts selected per token (enables sparse routing).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, intermediate_dim, num_experts=7, dropout=0.2, top_k=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k  \n",
    "\n",
    "        # Define the expert feed-forward sub-networks\n",
    "        self.experts = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_dim, intermediate_dim),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(intermediate_dim, hidden_dim)\n",
    "            ) for expert in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # Gating layer that determines expert weights\n",
    "        self.gate = torch.nn.Linear(hidden_dim, num_experts)\n",
    "\n",
    "        # Dropout applied after the expert-weighted output\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_logits = self.gate(x)  # Shape: (batch, seq_len, num_experts)\n",
    "\n",
    "        # Sparse top-k gating (only highest scoring experts are used)\n",
    "        if self.top_k > 0 and self.top_k < self.num_experts:\n",
    "            topk_values, topk_indices = torch.topk(gate_logits, self.top_k, dim=-1)\n",
    "            mask = torch.full_like(gate_logits, float('-inf'))\n",
    "            mask.scatter_(-1, topk_indices, topk_values)\n",
    "            gate_weights = torch.nn.functional.softmax(mask, dim=-1)\n",
    "        else:\n",
    "            gate_weights = torch.nn.functional.softmax(gate_logits, dim=-1)  # Use all experts if no top-k\n",
    "\n",
    "        # Compute output from each expert\n",
    "        expert_outputs = [expert(x) for expert in self.experts]  # List of (batch, seq_len, hidden_dim)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=2)      # Shape: (batch, seq_len, num_experts, hidden_dim)\n",
    "\n",
    "        gate_weights = gate_weights.unsqueeze(-1)                # Shape: (batch, seq_len, num_experts, 1)\n",
    "        weighted_output = expert_outputs * gate_weights          # Apply gating weights to expert outputs\n",
    "        output = weighted_output.sum(dim=2)                      # Sum across experts\n",
    "\n",
    "        return self.dropout(output), gate_weights.squeeze(-1)    # Return final output and gating weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d623f0-8a8e-46bb-8cac-a753e80d810f",
   "metadata": {},
   "source": [
    "### LongformerLayerWithMoE: Custom Longformer Layer with MoE-FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb663db-aba0-4434-bc3b-2bf2125ec0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongformerLayerWithMoE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A modified Longformer encoder layer that replaces the standard feed-forward network (FFN)\n",
    "    with a Mixture-of-Experts (MoE) block. Retains Longformer's self-attention mechanism.\n",
    "\n",
    "    Parameters:\n",
    "        config (LongformerConfig): Longformer model configuration.\n",
    "        num_experts (int): Number of experts in the MoE block.\n",
    "        top_k (int): Number of experts selected per token (enables sparse routing).\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_experts=7, top_k=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = LongformerSelfAttention(config)          # Sliding window attention\n",
    "        self.attention_output = LongformerSelfOutput(config)      # Residual + layer norm after attention\n",
    "\n",
    "        # Replace original FFN with Mixture-of-Experts feed-forward layer\n",
    "        self.intermediate = MoEFeedForward(\n",
    "            num_experts=num_experts,\n",
    "            hidden_dim=config.hidden_size,\n",
    "            dropout=config.hidden_dropout_prob,\n",
    "            intermediate_dim=config.intermediate_size,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # Final transformation and normalization\n",
    "        self.output_dense = torch.nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.output_dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.output_norm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def initialize_experts_from_ffn(self, pretrained_ffn):\n",
    "        \"\"\"\n",
    "        Copies weights from a standard FFN into all MoE experts, with added noise to differentiate them.\n",
    "\n",
    "        Parameters:\n",
    "            pretrained_ffn (torch.nn.Sequential): Pretrained FFN (Linear → GELU → Linear).\n",
    "        \"\"\"\n",
    "        for expert in self.intermediate.experts:\n",
    "            expert[0].weight.data.copy_(pretrained_ffn[0].weight.data.clone())\n",
    "            expert[0].bias.data.copy_(pretrained_ffn[0].bias.data.clone())\n",
    "            expert[2].weight.data.copy_(pretrained_ffn[2].weight.data.clone())\n",
    "            expert[2].bias.data.copy_(pretrained_ffn[2].bias.data.clone())\n",
    "\n",
    "            # Add small noise to each expert's parameters to break symmetry\n",
    "            for param in expert.parameters():\n",
    "                param.data += 0.01 * torch.randn_like(param)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the Longformer encoder layer with MoE.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_states (Tensor): Input embeddings (batch, seq_len, hidden_dim).\n",
    "            attention_mask (Tensor, optional): Attention mask for sliding-window attention.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]:\n",
    "                - layer_output: Output after attention + MoE + residual connection\n",
    "                - gate_weights: Gating distribution from MoE (for interpretability)\n",
    "        \"\"\"\n",
    "        attention_output = self.attention(hidden_states, attention_mask)[0]       # Self-attention step\n",
    "        attention_output = self.attention_output(attention_output, hidden_states) # Residual + norm\n",
    "\n",
    "        moe_output, gate_weights = self.intermediate(attention_output)            # MoE FFN block\n",
    "\n",
    "        ffn_output = self.output_dense(moe_output)\n",
    "        ffn_output = self.output_dropout(ffn_output)\n",
    "        layer_output = self.output_norm(ffn_output + attention_output)            # Final residual + norm\n",
    "\n",
    "        return layer_output, gate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab83c91-7afb-4aab-afc4-9734f4127231",
   "metadata": {},
   "source": [
    "###  MoeLongformerModel: Full Longformer Encoder with MoE Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf48cd5-7704-487f-9956-d7ccfe5094d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeLongformerModel(LongformerPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Longformer model enhanced with Mixture-of-Experts (MoE) feed-forward layers.\n",
    "\n",
    "    This class replaces each standard FFN block with a MoE layer and optionally \n",
    "    initializes those experts using weights from a pretrained Longformer model.\n",
    "\n",
    "    Parameters:\n",
    "        config (LongformerConfig): Hugging Face configuration object.\n",
    "        num_experts (int): Number of expert networks in each MoE block.\n",
    "        top_k (int): Number of experts selected per token (enables sparse routing).\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_experts=7, top_k=4):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = LongformerEmbeddings(config)  # Token + positional embeddings\n",
    "\n",
    "        # Replace standard encoder layers with MoE-enhanced ones\n",
    "        self.encoder_layers = torch.nn.ModuleList([\n",
    "            LongformerLayerWithMoE(config, num_experts=num_experts, top_k=top_k)\n",
    "            for hidden_layer in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        self.init_weights()  # Initialize model weights\n",
    "\n",
    "        # Load a pretrained Longformer model for expert initialization\n",
    "        base_longformer = LongformerModel(config)\n",
    "        base_longformer.eval()\n",
    "\n",
    "        for i, moe_layer in enumerate(self.encoder_layers):\n",
    "            # Extract the FFN sublayers from pretrained Longformer layer\n",
    "            intermediate = base_longformer.encoder.layer[i].intermediate.dense\n",
    "            output = base_longformer.encoder.layer[i].output.dense\n",
    "\n",
    "            # Wrap it into a structure matching the MoE FFN format\n",
    "            pretrained_ffn = torch.nn.Sequential(\n",
    "                intermediate,\n",
    "                torch.nn.GELU(),\n",
    "                output\n",
    "            )\n",
    "\n",
    "            # Initialize each expert with the same pretrained FFN (adds noise internally)\n",
    "            moe_layer.initialize_experts_from_ffn(pretrained_ffn)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the MoE-enhanced Longformer.\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (Tensor): Input token IDs (batch, seq_len).\n",
    "            attention_mask (Tensor, optional): Attention mask indicating valid tokens.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, List[Tensor]]:\n",
    "                - hidden_states: Final hidden states from the encoder\n",
    "                - pooled_output: [CLS]-like representation from first token\n",
    "                - gate_weights_list: List of expert gating weights from each layer\n",
    "        \"\"\"\n",
    "        embedding_output = self.embeddings(input_ids=input_ids)\n",
    "\n",
    "        # Convert to extended attention mask format used by Longformer\n",
    "        if attention_mask is not None:\n",
    "            extended_attention_mask = attention_mask[:, None, None, :]\n",
    "            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        else:\n",
    "            extended_attention_mask = None\n",
    "\n",
    "        hidden_states = embedding_output\n",
    "        gate_weights_list = []  # Store gating weights from all layers\n",
    "\n",
    "        # Pass input through each MoE-enhanced encoder layer\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states, gate_weights = layer(hidden_states, attention_mask=extended_attention_mask)\n",
    "            gate_weights_list.append(gate_weights)\n",
    "\n",
    "        pooled_output = hidden_states[:, 0]  # Use first token (like [CLS]) for pooled output\n",
    "\n",
    "        return hidden_states, pooled_output, gate_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cbae92-a855-4052-8bbc-73467791f30d",
   "metadata": {},
   "source": [
    "### MoeLongformerScorer: Essay Scoring Head with MoE Expert Diagnostics & Optional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c72a2-be3d-4b92-acd3-0d0c4c0dc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeLongformerScorer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Regression head for MoeLongformerModel, tailored for tasks like automated essay scoring.\n",
    "\n",
    "    Supports:\n",
    "    - Additional handcrafted features\n",
    "    - Entropy-based auxiliary loss to regularize expert usage\n",
    "    - Gate usage diagnostics for interpretability\n",
    "\n",
    "    Parameters:\n",
    "        base_model (MoeLongformerModel): MoE-enhanced Longformer encoder.\n",
    "        dropout (float): Dropout rate for regression head.\n",
    "        feature_dim (int): Size of additional handcrafted features.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: MoeLongformerModel, dropout=0.2, feature_dim=0):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model                      # Longformer encoder with MoE layers\n",
    "        self.feature_dim = feature_dim                 # Optional handcrafted feature dimension\n",
    "\n",
    "        input_dim = self.encoder.config.hidden_size + feature_dim  # Final input size to the regression head\n",
    "\n",
    "        # Regression head to predict a single scalar (score)\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 1),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, features=None, labels=None, aux_loss_weight=0.5):\n",
    "        \"\"\"\n",
    "        Forward pass for essay scoring.\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (Tensor): Input token IDs.\n",
    "            attention_mask (Tensor): Attention mask (1 for real tokens, 0 for padding).\n",
    "            features (Tensor, optional): Handcrafted features to concatenate to [CLS] representation.\n",
    "            labels (Tensor, optional): True scores for supervised learning.\n",
    "            aux_loss_weight (float): Weight for auxiliary loss on expert routing entropy.\n",
    "\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'loss': total training loss (MSE + aux),\n",
    "                'logits': predicted scores,\n",
    "                'hidden_states': transformer outputs,\n",
    "                'aux_loss': expert routing entropy penalty (optional)\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Run inputs through MoE Longformer encoder\n",
    "        hidden_states, pooled_output, gate_weights_list = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        last_gate = gate_weights_list[-1]  # Gating weights from final encoder layer\n",
    "\n",
    "        # Track expert usage stats and entropy for auxiliary loss\n",
    "        if aux_loss_weight > 0 and last_gate.ndim == 3:\n",
    "            usage_mask = (last_gate > 0).float()  # Mask of activated experts\n",
    "            usage_count = usage_mask.sum(dim=(0, 1))  # Count of times each expert is used\n",
    "            self.expert_usage_counts = usage_count.detach().cpu()\n",
    "\n",
    "            # Compute entropy of expert selection distribution\n",
    "            prob_dist = self.expert_usage_counts / self.expert_usage_counts.sum()\n",
    "            self.expert_entropy = -(prob_dist * torch.log(prob_dist + 1e-8)).sum().item()\n",
    "\n",
    "        # Save averaged gate weights for later visualization\n",
    "        self.last_gate_weights = gate_weights_list[-1].mean(dim=1).detach().cpu()\n",
    "\n",
    "        # Concatenate handcrafted features (if available)\n",
    "        if features is not None:\n",
    "            pooled_output = torch.cat([pooled_output, features], dim=-1)\n",
    "\n",
    "        # Predict final score\n",
    "        score = self.regressor(pooled_output).squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        # Compute training loss if labels provided\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.functional.mse_loss(score, labels.float())  # Main regression loss\n",
    "\n",
    "            if gate_weights_list:\n",
    "                gate_weights = gate_weights_list[-1]\n",
    "                mean_gates = gate_weights.mean(dim=(0, 1))  # Average gate probabilities across batch and time\n",
    "\n",
    "                entropy = -(mean_gates * torch.log(mean_gates + 1e-9)).sum()  # Entropy penalty\n",
    "                aux_loss = -entropy\n",
    "\n",
    "                loss = loss + aux_loss_weight * aux_loss  # Add weighted auxiliary loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": score,\n",
    "            \"hidden_states\": hidden_states,\n",
    "            \"aux_loss\": aux_loss if labels is not None else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c341aa0-909c-4d66-8427-7417d591d03d",
   "metadata": {},
   "source": [
    "### freeze_longformer_layers: Unfreeze Only Top Layers for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b6189-14c1-4642-8a8e-1abe5a03888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_longformer_layers(model, num_unfrozen=2):\n",
    "    \"\"\"\n",
    "    Freezes all layers in a Longformer-based model except for the top `num_unfrozen` encoder layers.\n",
    "\n",
    "    This helps reduce training cost and overfitting by only tuning the highest (most task-specific) layers.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): A Longformer or MoeLongformer model.\n",
    "        num_unfrozen (int): Number of top encoder layers to keep trainable.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # Freeze all parameters by default\n",
    "\n",
    "    # Unfreeze the last `num_unfrozen` encoder layers if available\n",
    "    if hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):\n",
    "        total_layers = len(model.encoder.layer)\n",
    "        for i in range(total_layers - num_unfrozen, total_layers):\n",
    "            for param in model.encoder.layer[i].parameters():\n",
    "                param.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aes_env]",
   "language": "python",
   "name": "conda-env-aes_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
