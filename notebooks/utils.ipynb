{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866c8ae3-7efe-4a7e-b88b-a4bee6765031",
   "metadata": {},
   "source": [
    "# Utils.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907480e-c906-4834-9e7d-6d660d8e45b5",
   "metadata": {},
   "source": [
    "### Imports and Language Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087f06f-bd58-4fa9-aadc-06268b2a7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from collections import Counter  \n",
    "from textblob import TextBlob  \n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import textstat  \n",
    "import spacy  \n",
    "import math  \n",
    "import re  \n",
    "\n",
    "# Load a lightweight English NLP model from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a150b5-c67f-4688-b675-b3580e3b9b55",
   "metadata": {},
   "source": [
    "### Normalize Essay Scores by Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb40b27-364b-4784-920e-d4311ccbbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_score(df):\n",
    "    \"\"\"\n",
    "    Normalize essay scores within each essay set to a 0–1 range.\n",
    "\n",
    "    Parameters:\n",
    "        df: A dataframe containing at least 'essay_set' and 'domain1_score' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original dataframe with added columns for min, max, and normalized scores.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Avoid modifying the original dataframe\n",
    "\n",
    "    # Calculate the minimum score for each essay set (prompt)\n",
    "    df[\"score_min\"] = df.groupby(\"essay_set\")[\"domain1_score\"].transform(\"min\")\n",
    "\n",
    "    # Calculate the maximum score for each essay set\n",
    "    df[\"score_max\"] = df.groupby(\"essay_set\")[\"domain1_score\"].transform(\"max\")\n",
    "\n",
    "    # Normalize the score to a range of 0–1 based on the min and max for the same essay set\n",
    "    df[\"normalized_score\"] = (df[\"domain1_score\"] - df[\"score_min\"]) / (df[\"score_max\"] - df[\"score_min\"])\n",
    "\n",
    "    return df  # Return the updated dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92be66-cc91-4b0e-8325-e3d43ae25664",
   "metadata": {},
   "source": [
    "### Extract Basic Length-Based Features from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2021d-14e3-4f12-9879-976fcbc2e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_length_features(text):\n",
    "    \"\"\"\n",
    "    Extract basic length-related features from a given text, including counts and averages.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The raw essay text.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the number of sentences, number of words,\n",
    "              average sentence length, and average word length.\n",
    "    \"\"\"\n",
    "    # Split text into sentences using punctuation (., !, ?) as delimiters\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty or whitespace-only sentences\n",
    "\n",
    "    # Extract all word-like tokens (alphanumeric) from the text\n",
    "    words = re.findall(r'\\w+', text)\n",
    "\n",
    "    # Count the number of sentences and words\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Calculate average sentence length (in words), avoid division by zero\n",
    "    avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    # Calculate average word length (in characters), avoid division by zero\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "    return {\n",
    "        \"num_sentences\": num_sentences,\n",
    "        \"num_words\": num_words,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"avg_word_length\": avg_word_length\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b247fc3-e2da-44c5-b1d5-885cbca44301",
   "metadata": {},
   "source": [
    "### Extract Readability Metrics from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e829410-0318-4d33-a15f-6172cd9dfb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_readability_features(text):\n",
    "    \"\"\"\n",
    "    Compute standard readability scores for a given text.\n",
    "\n",
    "    These scores help estimate how difficult a text is to read, \n",
    "    often reflecting the education level required to understand it.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input essay or paragraph.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of various readability metrics.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text),              \n",
    "        # Higher is easier to read; typically ranges from 0 (hard) to 100 (easy)\n",
    "\n",
    "        \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text),            \n",
    "        # U.S. school grade level required to understand the text\n",
    "\n",
    "        \"gunning_fog\": textstat.gunning_fog(text),                              \n",
    "        # Estimates years of formal education needed\n",
    "\n",
    "        \"smog_index\": textstat.smog_index(text),                                \n",
    "        # Focuses on polysyllabic words; suitable for short texts\n",
    "\n",
    "        \"dale_chall\": textstat.dale_chall_readability_score(text),              \n",
    "        # Considers difficult words based on a predefined list\n",
    "\n",
    "        \"automated_readability\": textstat.automated_readability_index(text)     \n",
    "        # Based on characters per word and words per sentence\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb705db8-7b95-448a-b1b7-6a70608ca910",
   "metadata": {},
   "source": [
    "### Extract Text Complexity Features Using Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631a702-3fb1-4e11-a657-ad40f6db471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_complexity_features(text):\n",
    "    \"\"\"\n",
    "    Analyze the syntactic structure of text to extract complexity-related features\n",
    "    using dependency parse trees.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "              - average parse tree depth\n",
    "              - average leaf node depth\n",
    "              - number of clauses\n",
    "              - average clause length\n",
    "              - maximum clause length\n",
    "    \"\"\"\n",
    "    doc = nlp(text)  # Use spaCy to tokenize and parse the text\n",
    "\n",
    "    tree_depths = []     # Stores depth of each sentence's parse tree\n",
    "    leaf_depths = []     # Stores depth of each leaf node in the tree\n",
    "    clause_counts = []   # Stores number of clauses per sentence\n",
    "    clause_lengths = []  # Stores lengths of individual clauses\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        # Find the root of the dependency tree for the sentence\n",
    "        roots = [token for token in sent if token.head == token] \n",
    "        if not roots:\n",
    "            continue\n",
    "        root = roots[0]  \n",
    "\n",
    "        # Recursive function to calculate depth of a token's subtree\n",
    "        def get_depth(token):\n",
    "            if not list(token.children):\n",
    "                return 1\n",
    "            return 1 + max(get_depth(child) for child in token.children)\n",
    "\n",
    "        # Recursive function to collect depths of all leaf nodes\n",
    "        def get_leaf_depths(token, depth=1):\n",
    "            if not list(token.children):\n",
    "                return [depth]\n",
    "            return sum([get_leaf_depths(child, depth + 1) for child in token.children], [])\n",
    "\n",
    "        tree_depths.append(get_depth(root))             # Depth of full sentence tree\n",
    "        leaf_depths += get_leaf_depths(root)            # Collect all leaf depths\n",
    "\n",
    "        # Count specific types of subordinate clauses\n",
    "        clauses = [tok for tok in sent if tok.dep_ in (\"ccomp\", \"advcl\", \"acl\", \"relcl\", \"xcomp\", \"mark\")]\n",
    "        clause_counts.append(len(clauses))              # Number of clauses in the sentence\n",
    "\n",
    "        # Calculate number of tokens in each clause\n",
    "        clause_lengths.extend([len(list(clause.subtree)) for clause in clauses])\n",
    "\n",
    "    # Return summary statistics capturing syntactic complexity\n",
    "    return {\n",
    "        \"avg_parse_tree_depth\": sum(tree_depths) / len(tree_depths) if tree_depths else 0,\n",
    "        \"avg_leaf_depth\": sum(leaf_depths) / len(leaf_depths) if leaf_depths else 0,\n",
    "        \"num_clauses\": sum(clause_counts),\n",
    "        \"avg_clause_length\": sum(clause_lengths) / len(clause_lengths) if clause_lengths else 0,\n",
    "        \"max_clause_length\": max(clause_lengths) if clause_lengths else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c25b4-9e9f-400f-8ca5-6cf85ab65708",
   "metadata": {},
   "source": [
    "### Extract Linguistic Variation Features from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fef2c-e06b-4d19-a653-a6859668d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_variation_features(text):\n",
    "    \"\"\"\n",
    "    Compute features that capture vocabulary richness and grammatical diversity.\n",
    "\n",
    "    These metrics reflect how varied the writing is in terms of vocabulary and part-of-speech usage.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input essay text.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with:\n",
    "              - type-token ratio\n",
    "              - lexical density\n",
    "              - number of unique POS tags\n",
    "              - POS entropy\n",
    "              - repetition rate\n",
    "    \"\"\"\n",
    "    doc = nlp(text)  # Tokenize and tag using spaCy\n",
    "\n",
    "    # Lowercase alphabetic words (filtering out punctuation/numbers)\n",
    "    words = [token.text.lower() for token in doc if token.is_alpha]\n",
    "    unique_words = set(words)\n",
    "\n",
    "    # Collect part-of-speech (POS) tags\n",
    "    pos_tags = [token.pos_ for token in doc if token.is_alpha]\n",
    "    pos_counts = Counter(pos_tags)\n",
    "\n",
    "    # Count how many words are content-bearing (e.g., nouns, verbs, adjectives, adverbs)\n",
    "    content_tags = {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "    content_count = sum(1 for token in doc if token.pos_ in content_tags)\n",
    "\n",
    "    # Compute entropy of POS tag distribution to measure grammatical diversity\n",
    "    total_pos = sum(pos_counts.values())\n",
    "    pos_entropy = -sum(\n",
    "        (count / total_pos) * math.log2(count / total_pos)\n",
    "        for count in pos_counts.values()\n",
    "    ) if total_pos > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"type_token_ratio\": len(unique_words) / len(words) if words else 0,          \n",
    "        # Measures vocabulary diversity (higher = more diverse)\n",
    "\n",
    "        \"lexical_density\": content_count / len(words) if words else 0,              \n",
    "        # Proportion of content words (vs. function words)\n",
    "\n",
    "        \"num_unique_pos_tags\": len(pos_counts),                                      \n",
    "        # Number of distinct POS types used\n",
    "\n",
    "        \"pos_entropy\": pos_entropy,                                                  \n",
    "        # Reflects how evenly POS types are used (higher = more varied grammar)\n",
    "\n",
    "        \"repeat_rate\": (len(words) - len(unique_words)) / len(words) if words else 0 \n",
    "        # Measures word repetition (lower = more unique word usage)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c02f1-7a6d-4b5e-a105-c55550066ca8",
   "metadata": {},
   "source": [
    "### Extract Sentiment-Based Features from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cc3a2-06a4-4e81-b188-53083c50adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_features(text):\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of the text using TextBlob.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "              - sentiment polarity: how positive or negative the text is\n",
    "              - sentiment subjectivity: how subjective or opinionated the text is\n",
    "    \"\"\"\n",
    "    blob = TextBlob(text)  # Convert text into a TextBlob object for analysis\n",
    "\n",
    "    polarity = blob.sentiment.polarity       \n",
    "    # Polarity ranges from -1 (very negative) to 1 (very positive)\n",
    "\n",
    "    subjectivity = blob.sentiment.subjectivity  \n",
    "    # Subjectivity ranges from 0 (objective/factual) to 1 (highly subjective)\n",
    "\n",
    "    return {\n",
    "        \"sent_polarity\": polarity,\n",
    "        \"sent_subjectivity\": subjectivity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e7f1f-6d5d-4126-84ef-99443d66067f",
   "metadata": {},
   "source": [
    "### Add Normalized Feature Set to DataFrame (Per Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27073c-3e81-4331-8532-ddaddd716f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, func, prefix):\n",
    "    \"\"\"\n",
    "    Apply a feature-extraction function to each essay and normalize the resulting features\n",
    "    per essay prompt (essay_set). Appends the normalized features back to the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Original dataframe containing at least 'essay' and 'essay_set' columns.\n",
    "        func (function): A function that extracts features from a single essay (returns a dict).\n",
    "        prefix (str): A string prefix to add to the new feature column names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input dataframe with new normalized feature columns added.\n",
    "    \"\"\"\n",
    "    # Apply the feature extraction function to each essay\n",
    "    type_feats = df[\"essay\"].apply(func)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    type_df = pd.DataFrame(list(type_feats))\n",
    "    type_df[\"essay_set\"] = df[\"essay_set\"]  # Keep track of which prompt each essay belongs to\n",
    "\n",
    "    scalers = {}       # Will store a separate scaler for each prompt\n",
    "    normalized = []    # Will store the normalized feature sub-dataframes for each prompt\n",
    "\n",
    "    for prompt in df[\"essay_set\"].unique():\n",
    "        # Select only the rows for the current prompt, excluding the prompt label\n",
    "        sub = type_df[type_df[\"essay_set\"] == prompt].drop(columns=\"essay_set\")\n",
    "\n",
    "        # Fit a MinMaxScaler on this subset and transform the features to range [0, 1]\n",
    "        scaler = MinMaxScaler()\n",
    "        norm_sub = scaler.fit_transform(sub)\n",
    "\n",
    "        # Store the normalized DataFrame (retain original index)\n",
    "        normalized.append(pd.DataFrame(norm_sub, columns=sub.columns, index=sub.index))\n",
    "\n",
    "        # Save the scaler in case it's needed later (e.g., for inference)\n",
    "        scalers[prompt] = scaler\n",
    "\n",
    "    # Combine all normalized feature DataFrames and restore original ordering\n",
    "    type_df_normalized = pd.concat(normalized).sort_index()\n",
    "\n",
    "    # Add the normalized features to the original DataFrame with a prefix\n",
    "    df = pd.concat([df, type_df_normalized.add_prefix(prefix)], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d2a8f-f18e-4d99-a2b7-e5654fb8df48",
   "metadata": {},
   "source": [
    "### Convert Normalized Scores Back to Original Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8e638-8cad-443b-8b5a-53f7fb860df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_score(normalized_score, df):\n",
    "    \"\"\"\n",
    "    Convert normalized scores (ranging from 0 to 1) back to their original scale \n",
    "    using the min and max score values from the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        normalized_score (np.array or list): Normalized scores between 0 and 1.\n",
    "        df (pd.DataFrame): DataFrame containing 'score_min' and 'score_max' columns.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Denormalized scores rounded to the nearest integer.\n",
    "    \"\"\"\n",
    "    normalized_score = np.squeeze(normalized_score)  # Ensure 1D array format if needed\n",
    "\n",
    "    # Retrieve original min and max scores (per sample) from the dataframe\n",
    "    min_score = df[\"score_min\"].values\n",
    "    max_score = df[\"score_max\"].values\n",
    "\n",
    "    # Scale normalized scores back to the original range\n",
    "    denormalized_score = normalized_score * (max_score - min_score) + min_score\n",
    "\n",
    "    return np.round(denormalized_score).astype(int)  # Round and convert to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46cc88-0b31-47fc-8c49-a22314ec3a0c",
   "metadata": {},
   "source": [
    "### Generate 7-Train / 1-Test Prompt Splits (Cross-Prompt Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8611eb-ab18-448c-be48-c348ab6043bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train7_test1_splits(all_sets=None):\n",
    "    \"\"\"\n",
    "    Generate 8-fold cross-prompt splits where each essay set (prompt) is used once as the test set,\n",
    "    and the remaining 7 sets are used for training.\n",
    "\n",
    "    Parameters:\n",
    "        all_sets (list, optional): A list of all essay set IDs (defaults to sets 1–8).\n",
    "\n",
    "    Returns:\n",
    "        list of dict: A list containing 8 dictionaries, each with a 'train' list and a 'test' ID.\n",
    "    \"\"\"\n",
    "    if all_sets is None:\n",
    "        all_sets = [1, 2, 3, 4, 5, 6, 7, 8]  # Default to 8 essay prompts if not provided\n",
    "\n",
    "    splits = []  # Will hold all (train, test) split configurations\n",
    "\n",
    "    for test_set in all_sets:\n",
    "        # Use all sets except the current one for training\n",
    "        train_sets = [s for s in all_sets if s != test_set]\n",
    "\n",
    "        # Add this split to the list\n",
    "        splits.append({\"train\": train_sets, \"test\": test_set})\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7ad10-614d-4f78-ad58-f9a36b2a1139",
   "metadata": {},
   "source": [
    "### Preprocess Example for Tokenization and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e5af5-ac37-46cf-a7ee-bff16501120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize essay text and extract handcrafted features for model input.\n",
    "\n",
    "    This function prepares each input example by:\n",
    "      - Tokenizing the essay using the given tokenizer\n",
    "      - Attaching the normalized score as the label\n",
    "      - Collecting handcrafted features (e.g., length, readability, etc.)\n",
    "\n",
    "    Parameters:\n",
    "        example (dict): A single data sample with keys like 'essay', 'normalized_score', and feature columns.\n",
    "        tokenizer (Tokenizer): A Hugging Face tokenizer (e.g., BERT tokenizer).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "              - tokenized inputs (input_ids, attention_mask, etc.)\n",
    "              - 'labels': the normalized score\n",
    "              - 'features': a list of handcrafted feature values\n",
    "    \"\"\"\n",
    "    # Tokenize the essay with truncation and padding for fixed-length inputs\n",
    "    tokens = tokenizer(example[\"essay\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Assign the normalized essay score as the label\n",
    "    tokens[\"labels\"] = example[\"normalized_score\"]\n",
    "\n",
    "    # Define prefixes for handcrafted feature columns\n",
    "    feature_prefixes = (\"len_\", \"read_\", \"comp_\", \"var_\", \"sent_\")\n",
    "\n",
    "    # Collect feature values that match the desired prefixes\n",
    "    handcrafted_feats = [value for key, value in example.items() if key.startswith(feature_prefixes)]\n",
    "\n",
    "    # Add handcrafted features to the tokenized output\n",
    "    tokens[\"features\"] = handcrafted_feats\n",
    "\n",
    "    return tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aes_env]",
   "language": "python",
   "name": "conda-env-aes_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
